{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of High Dimensional Data Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/DS-Unit-2-Sprint-1-Linear-Algebra/blob/master/DS-Unit-2-Sprint-1-Linear-Algebra/Copy_of_High_Dimensional_Data_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7wMWCkE1RZpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vertical Line Test"
      ]
    },
    {
      "metadata": {
        "id": "W0-g7aprRv2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Create two graphs, one that passes the vertical line test and one that does not."
      ]
    },
    {
      "metadata": {
        "id": "fIJhCtF6RW_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckeTKqMgRy7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Why are graphs that don't pass the vertical line test not considered \"functions?\""
      ]
    },
    {
      "metadata": {
        "id": "Gtl20YeDR6x-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because they can fail the \"uniqueness\" predicates, in other words, they would fail to be represented as python dictionaries. "
      ]
    },
    {
      "metadata": {
        "id": "g21uN62xSKSk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Functions as Relations"
      ]
    },
    {
      "metadata": {
        "id": "gwkcV-EMSMNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Which of the following relations are functions? Why?\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Relation 1: } \\{(1, 2), (3, 2), (1, 3)\\}\n",
        "\\\\\n",
        "\\text{Relation 2: } \\{(1, 3), (2, 3), (6, 7)\\}\n",
        "\\\\\n",
        "\\text{Relation 3: } \\{(9, 4), (2, 1), (9, 6)\\}\n",
        "\\\\\n",
        "\\text{Relation 4: } \\{(6, 2), (8, 3), (6, 4)\\}\n",
        "\\\\\n",
        "\\text{Relation 5: } \\{(2, 6), (2, 7), (2, 4)\\}\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "LVuBUu9Tvqvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1, 3, 4, 5; because these are those that fail the uniqueness predicate"
      ]
    },
    {
      "metadata": {
        "id": "y0U30PrlTAAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Functions as a mapping between dimensions\n"
      ]
    },
    {
      "metadata": {
        "id": "pw-OU9qmT5Ua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 for the following functions what is the dimensionality of the domain (input) and codomain (range/output)?\n",
        "\n",
        "\\begin{align}\n",
        "m(洧논_1,洧논_2,洧논_3)=(x_1+x_2, x_1+x_3, x_2+x_3)\n",
        "\\\\\n",
        "n(洧논_1,洧논_2,洧논_3,洧논_4)=(x_2^2 + x_3, x_2x_4)\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "LQW9NQBuzUmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$ m : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ \n",
        "\n",
        "$ n : \\mathbb{R}^4 \\rightarrow \\mathbb{R}^2$ "
      ]
    },
    {
      "metadata": {
        "id": "k4tKHjdHUevC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Do you think it's possible to create a function that maps from a lower dimensional space to a higher dimensional space? If so, provide an example."
      ]
    },
    {
      "metadata": {
        "id": "bGPvBgRTzt-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "goqoSLcsz7Bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def const_of_length_5(x): \n",
        "  # a function R -> R^5\n",
        "  return [x] * 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2nEWvwVyVWdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vector Transformations"
      ]
    },
    {
      "metadata": {
        "id": "1n0-6FsYVcVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 Plug the corresponding unit vectors into each function. Use the output vectors to create a transformation matrix.\n",
        "\n",
        "\\begin{align}\n",
        "p(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}) = \\begin{bmatrix} x_1 + 3x_2 \\\\2 x_2 - x_1 \\\\  \\end{bmatrix}\n",
        "\\\\\n",
        "\\\\\n",
        "q(\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix}) = \\begin{bmatrix} 4x_1 + x_2 + 2x_3 \\\\2 x_2 - x_1 + 3x_3 \\\\ 5x_1 - 2x_3 + x_2  \\end{bmatrix}\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "NwL4D5SHyiCv",
        "colab_type": "code",
        "outputId": "3951d63f-82a4-4b80-d489-52b69a01b436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def p(x): \n",
        "  return [x[0] + 3*x[1], 2*x[1] - x[0]]\n",
        "\n",
        "Matrix_p = np.asmatrix([p([1,0]), p([0,1])]).T\n",
        "\n",
        "Matrix_p\n",
        "\n",
        "def q(x): \n",
        "  return [4*x[0] + x[1] + 2*x[2], 2*x[1] - x[0] + 3*x[2], 5*x[0] - 2*x[2] + x[1]]\n",
        "\n",
        "Matrix_q = np.asmatrix([q([1,0,0]), q([0,1,0]), q([0,0,1])]).T\n",
        "\n",
        "Matrix_q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 4,  1,  2],\n",
              "        [-1,  2,  3],\n",
              "        [ 5,  1, -2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "n5HUOQIxZ2gp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 Verify that your transformation matrices are correct by choosing an input matrix and calculating the result both via the traditional functions above and also via vector-matrix multiplication."
      ]
    },
    {
      "metadata": {
        "id": "0UANR1IEaVWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import pi, e\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "w1 = [pi, e]\n",
        "w2 = [pi, pi-e, e]\n",
        "\n",
        "assert_almost_equal(np.asmatrix(p(w1)), np.dot(Matrix_p, w1))\n",
        "\n",
        "assert_almost_equal(np.asmatrix(q(w2)), np.dot(Matrix_q, w2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEoiri3mak7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Eigenvalues and Eigenvectors"
      ]
    },
    {
      "metadata": {
        "id": "5HY0R4u7anIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 In your own words, give an explanation for the intuition behind eigenvalues and eigenvectors."
      ]
    },
    {
      "metadata": {
        "id": "yXZi-k9i0abw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the thing i'm still working on is the difference between eigenvec or val and determinant, because determinant is a scalar that measures how much the transformation stretched/squished unit area, and eigenvalue is a scalar that measures how much the transformation stretched/squished an eigenvector... But if they're this similar, i feel like i should be able to construct determinant from some operation on eigenthings. --- also, i keep forgetting why eigenvectors are special (and they're not just basis vectors)\n",
        "\n",
        "oh wait yeah i just remembered that eigenvectors form the basis in which this transformation is most drastic"
      ]
    },
    {
      "metadata": {
        "id": "VdehMwBtatKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Curse of Dimensionality"
      ]
    },
    {
      "metadata": {
        "id": "4oVrJax-a3SK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 What are some of the challenges of working with high dimensional spaces?\n",
        "\n",
        "Computational cost, illegibility. \n",
        "\n",
        "Really it can get intractable for both the computers and the humans involved, not good. "
      ]
    },
    {
      "metadata": {
        "id": "x0gE7phb035_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiBJxsZla88c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 What is the rule of thumb for how many observations you should have compared to parameters in your model?\n",
        "\n",
        "for N observations of M features, you **must** have *at least* N>M, and a heuristic like roughly N>5M is preferred. "
      ]
    },
    {
      "metadata": {
        "id": "NZmqdeygbHJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis"
      ]
    },
    {
      "metadata": {
        "id": "7iEheetpbJdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.1 Load the UCI Machine Learning Repository's [Iris Dataset](https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv) and use PCA to isolate the dataset's first and second principal components and plot them on a graph. "
      ]
    },
    {
      "metadata": {
        "id": "wy9AZVgZcHLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "be56ecb2-1b63-4cd0-bd98-b426d701fc4c"
      },
      "cell_type": "code",
      "source": [
        "ur = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(ur).select_dtypes(include='float64')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width\n",
              "0           5.1          3.5           1.4          0.2\n",
              "1           4.9          3.0           1.4          0.2\n",
              "2           4.7          3.2           1.3          0.2\n",
              "3           4.6          3.1           1.5          0.2\n",
              "4           5.0          3.6           1.4          0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Oz2pnVXDk6E0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "7a927933-3175-4e3b-ff5a-e192707ca400"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import altair as alt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "pca.fit(df)\n",
        "\n",
        "flowers_pca2 = pd.DataFrame(pca.transform(df), columns=['PC 1', 'PC 2'])\n",
        "\n",
        "C = alt.Chart(flowers_pca2).mark_circle().encode(x='PC 1', y='PC 2')\n",
        "\n",
        "C"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chart({\n",
              "  data:          PC 1      PC 2\n",
              "  0   -2.684207  0.326607\n",
              "  1   -2.715391 -0.169557\n",
              "  2   -2.889820 -0.137346\n",
              "  3   -2.746437 -0.311124\n",
              "  4   -2.728593  0.333925\n",
              "  5   -2.279897  0.747783\n",
              "  6   -2.820891 -0.082105\n",
              "  7   -2.626482  0.170405\n",
              "  8   -2.887959 -0.570798\n",
              "  9   -2.673845 -0.106692\n",
              "  10  -2.506527  0.651935\n",
              "  11  -2.613143  0.021521\n",
              "  12  -2.787434 -0.227740\n",
              "  13  -3.225200 -0.503280\n",
              "  14  -2.643543  1.186195\n",
              "  15  -2.383869  1.344754\n",
              "  16  -2.622526  0.818090\n",
              "  17  -2.648323  0.319137\n",
              "  18  -2.199078  0.879244\n",
              "  19  -2.587346  0.520474\n",
              "  20  -2.310532  0.397868\n",
              "  21  -2.543235  0.440032\n",
              "  22  -3.215858  0.141616\n",
              "  23  -2.303129  0.105523\n",
              "  24  -2.356171 -0.031210\n",
              "  25  -2.507917 -0.139056\n",
              "  26  -2.469056  0.137887\n",
              "  27  -2.562391  0.374685\n",
              "  28  -2.639821  0.319290\n",
              "  29  -2.632848 -0.190076\n",
              "  ..        ...       ...\n",
              "  120  2.428167  0.376782\n",
              "  121  1.198097 -0.605579\n",
              "  122  3.499265  0.456773\n",
              "  123  1.387668 -0.204031\n",
              "  124  2.275854  0.333387\n",
              "  125  2.614194  0.558367\n",
              "  126  1.257625 -0.179137\n",
              "  127  1.290670 -0.116425\n",
              "  128  2.122854 -0.210855\n",
              "  129  2.387564  0.462519\n",
              "  130  2.840961  0.372743\n",
              "  131  3.232343  1.370524\n",
              "  132  2.158738 -0.218326\n",
              "  133  1.443103 -0.143801\n",
              "  134  1.779640 -0.501465\n",
              "  135  3.076522  0.685764\n",
              "  136  2.144987  0.138907\n",
              "  137  1.904863  0.048048\n",
              "  138  1.168853 -0.164502\n",
              "  139  2.107654  0.371482\n",
              "  140  2.314303  0.182609\n",
              "  141  1.922451  0.409271\n",
              "  142  1.414072 -0.574925\n",
              "  143  2.563323  0.275975\n",
              "  144  2.419391  0.303504\n",
              "  145  1.944017  0.187415\n",
              "  146  1.525664 -0.375021\n",
              "  147  1.764046  0.078519\n",
              "  148  1.901629  0.115877\n",
              "  149  1.389666 -0.282887\n",
              "  \n",
              "  [150 rows x 2 columns],\n",
              "  encoding: EncodingWithFacet({\n",
              "    x: X({\n",
              "      shorthand: 'PC 1'\n",
              "    }),\n",
              "    y: Y({\n",
              "      shorthand: 'PC 2'\n",
              "    })\n",
              "  }),\n",
              "  mark: 'circle'\n",
              "})"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@4\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@3\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "      var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-80e2677441053254c757971d74e90158\"}, \"mark\": \"circle\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"PC 1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"PC 2\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-80e2677441053254c757971d74e90158\": [{\"PC 1\": -2.6842071251039483, \"PC 2\": 0.32660731476438803}, {\"PC 1\": -2.715390615634131, \"PC 2\": -0.16955684755602612}, {\"PC 1\": -2.8898195396179163, \"PC 2\": -0.13734560960502776}, {\"PC 1\": -2.746437197308735, \"PC 2\": -0.3111243157519919}, {\"PC 1\": -2.728592981831315, \"PC 2\": 0.33392456356845474}, {\"PC 1\": -2.2798973610095974, \"PC 2\": 0.7477827132251336}, {\"PC 1\": -2.82089068218063, \"PC 2\": -0.08210451102468097}, {\"PC 1\": -2.626481993323819, \"PC 2\": 0.1704053489602899}, {\"PC 1\": -2.8879585653356346, \"PC 2\": -0.5707980263315917}, {\"PC 1\": -2.673844686719121, \"PC 2\": -0.10669170375273852}, {\"PC 1\": -2.506526789338903, \"PC 2\": 0.651935013672572}, {\"PC 1\": -2.613142718271056, \"PC 2\": 0.021520631960257997}, {\"PC 1\": -2.7874339759970974, \"PC 2\": -0.22774018887110628}, {\"PC 1\": -3.2252004462749806, \"PC 2\": -0.5032799094854259}, {\"PC 1\": -2.6435432169411466, \"PC 2\": 1.1861948994134497}, {\"PC 1\": -2.383869323799375, \"PC 2\": 1.3447543445598624}, {\"PC 1\": -2.6225262031258083, \"PC 2\": 0.8180896745965953}, {\"PC 1\": -2.6483227324791265, \"PC 2\": 0.31913666775088473}, {\"PC 1\": -2.199077961430763, \"PC 2\": 0.8792440880917366}, {\"PC 1\": -2.587346188917738, \"PC 2\": 0.520473638805968}, {\"PC 1\": -2.3105317013131335, \"PC 2\": 0.397867821588892}, {\"PC 1\": -2.5432349073036953, \"PC 2\": 0.4400317546598153}, {\"PC 1\": -3.2158576949001056, \"PC 2\": 0.14161557162558355}, {\"PC 1\": -2.3031285376638824, \"PC 2\": 0.10552267842998225}, {\"PC 1\": -2.3561710866838976, \"PC 2\": -0.031209589068338317}, {\"PC 1\": -2.5079172268378804, \"PC 2\": -0.13905633991317412}, {\"PC 1\": -2.469055997545123, \"PC 2\": 0.13788731459041773}, {\"PC 1\": -2.5623909468367505, \"PC 2\": 0.3746845627501061}, {\"PC 1\": -2.639821268376582, \"PC 2\": 0.31929006596032183}, {\"PC 1\": -2.632847908030758, \"PC 2\": -0.19007583063362407}, {\"PC 1\": -2.588462051303392, \"PC 2\": -0.19739307943769077}, {\"PC 1\": -2.4100773371215958, \"PC 2\": 0.4180800082476162}, {\"PC 1\": -2.6476366733969003, \"PC 2\": 0.8199826325595071}, {\"PC 1\": -2.5971594770759183, \"PC 2\": 1.1000219280072687}, {\"PC 1\": -2.673844686719121, \"PC 2\": -0.10669170375273852}, {\"PC 1\": -2.8669998469325346, \"PC 2\": 0.07719309572358718}, {\"PC 1\": -2.625228464680421, \"PC 2\": 0.6068000084215864}, {\"PC 1\": -2.673844686719121, \"PC 2\": -0.10669170375273852}, {\"PC 1\": -2.9818426648539083, \"PC 2\": -0.48025004885607664}, {\"PC 1\": -2.590323025585674, \"PC 2\": 0.23605933728887285}, {\"PC 1\": -2.770138910746324, \"PC 2\": 0.2710594197651671}, {\"PC 1\": -2.852211081566392, \"PC 2\": -0.9328653674695447}, {\"PC 1\": -2.998296442832351, \"PC 2\": -0.33430757459077726}, {\"PC 1\": -2.4055141012847012, \"PC 2\": 0.19591725769606075}, {\"PC 1\": -2.2088329541767053, \"PC 2\": 0.4426960304210029}, {\"PC 1\": -2.7156651907474543, \"PC 2\": -0.24268148289811295}, {\"PC 1\": -2.5375733710135067, \"PC 2\": 0.5103675454766059}, {\"PC 1\": -2.840321296827009, \"PC 2\": -0.22057633827647674}, {\"PC 1\": -2.5426857570770482, \"PC 2\": 0.5862810253439885}, {\"PC 1\": -2.7039123148636506, \"PC 2\": 0.1150108521705056}, {\"PC 1\": 1.2847945878450715, \"PC 2\": 0.6854391861329201}, {\"PC 1\": 0.9324107529829178, \"PC 2\": 0.3191980898336488}, {\"PC 1\": 1.4640613227790755, \"PC 2\": 0.5041898329724532}, {\"PC 1\": 0.1809672063476987, \"PC 2\": -0.8256039435761133}, {\"PC 1\": 1.0871344872070008, \"PC 2\": 0.07539038928876755}, {\"PC 1\": 0.6404367495231457, \"PC 2\": -0.4173234829700258}, {\"PC 1\": 1.0952237099384787, \"PC 2\": 0.2838912109384804}, {\"PC 1\": -0.7514671406482268, \"PC 2\": -1.0011075129743943}, {\"PC 1\": 1.043297780706281, \"PC 2\": 0.2289569087770069}, {\"PC 1\": -0.010190070727853176, \"PC 2\": -0.7205748667019017}, {\"PC 1\": -0.5110861958950905, \"PC 2\": -1.2624919538621409}, {\"PC 1\": 0.5110980606834776, \"PC 2\": -0.10228410504597023}, {\"PC 1\": 0.2623357561531802, \"PC 2\": -0.5478932980253367}, {\"PC 1\": 0.9840445451694312, \"PC 2\": -0.12436042022227793}, {\"PC 1\": -0.17486400196569613, \"PC 2\": -0.2518155710801706}, {\"PC 1\": 0.9275729420326999, \"PC 2\": 0.46823620504311736}, {\"PC 1\": 0.6595927890562007, \"PC 2\": -0.3519762910603166}, {\"PC 1\": 0.23454058625983565, \"PC 2\": -0.3319218293621204}, {\"PC 1\": 0.942361707398842, \"PC 2\": -0.5418222581500141}, {\"PC 1\": 0.04324640032870473, \"PC 2\": -0.5814894466123587}, {\"PC 1\": 1.1162407237538159, \"PC 2\": -0.08421401387837348}, {\"PC 1\": 0.35678656783046125, \"PC 2\": -0.06682382794136613}, {\"PC 1\": 1.2964688502855342, \"PC 2\": -0.32756151979494424}, {\"PC 1\": 0.9205026489090096, \"PC 2\": -0.18239036332792094}, {\"PC 1\": 0.7140082136428333, \"PC 2\": 0.15037915314843728}, {\"PC 1\": 0.8996408632837762, \"PC 2\": 0.329610979581884}, {\"PC 1\": 1.33104141885472, \"PC 2\": 0.24466952060228941}, {\"PC 1\": 1.5573962720707026, \"PC 2\": 0.26739258481276496}, {\"PC 1\": 0.8124555489980021, \"PC 2\": -0.1623315748786333}, {\"PC 1\": -0.3073347556634043, \"PC 2\": -0.3650866127661604}, {\"PC 1\": -0.07034288894927156, \"PC 2\": -0.7025379317307259}, {\"PC 1\": -0.19188449210314565, \"PC 2\": -0.6774905443743572}, {\"PC 1\": 0.13499495045137336, \"PC 2\": -0.3117096427033962}, {\"PC 1\": 1.378736982775583, \"PC 2\": -0.42120513821462835}, {\"PC 1\": 0.587274853579911, \"PC 2\": -0.48328426771748256}, {\"PC 1\": 0.8072054966767166, \"PC 2\": 0.19505396377111153}, {\"PC 1\": 1.2204289662446797, \"PC 2\": 0.4080353370010177}, {\"PC 1\": 0.8128677903690699, \"PC 2\": -0.3706789983189097}, {\"PC 1\": 0.2451951616903461, \"PC 2\": -0.266728035661848}, {\"PC 1\": 0.16451342836925584, \"PC 2\": -0.679661469310814}, {\"PC 1\": 0.4630309888714244, \"PC 2\": -0.6695265465361226}, {\"PC 1\": 0.8901604456511565, \"PC 2\": -0.033812442746762664}, {\"PC 1\": 0.22887904996964767, \"PC 2\": -0.40225762017891137}, {\"PC 1\": -0.7070812839208606, \"PC 2\": -1.008424761778461}, {\"PC 1\": 0.3555330391870636, \"PC 2\": -0.5032184874026624}, {\"PC 1\": 0.33112694733272297, \"PC 2\": -0.21118014066262672}, {\"PC 1\": 0.37523822894676595, \"PC 2\": -0.29162202480877975}, {\"PC 1\": 0.6416902781665432, \"PC 2\": 0.019071176491270783}, {\"PC 1\": -0.9084633331234953, \"PC 2\": -0.7515687251694857}, {\"PC 1\": 0.2978079074069341, \"PC 2\": -0.34701652159856394}, {\"PC 1\": 2.5317269804395592, \"PC 2\": -0.011842236640300449}, {\"PC 1\": 1.4140722251737576, \"PC 2\": -0.5749250559123048}, {\"PC 1\": 2.6164846082840443, \"PC 2\": 0.3419352869872949}, {\"PC 1\": 1.970814945906482, \"PC 2\": -0.18112569470491374}, {\"PC 1\": 2.349757983950943, \"PC 2\": -0.04188254965484183}, {\"PC 1\": 3.396879920678138, \"PC 2\": 0.5471680462301528}, {\"PC 1\": 0.5193832450849365, \"PC 2\": -1.1913516890506533}, {\"PC 1\": 2.9320050969913014, \"PC 2\": 0.35237700618085993}, {\"PC 1\": 2.3196727938740533, \"PC 2\": -0.24554817060691017}, {\"PC 1\": 2.918134233644252, \"PC 2\": 0.7803806293720323}, {\"PC 1\": 1.661934947019487, \"PC 2\": 0.24203840103752242}, {\"PC 1\": 1.8023404526607332, \"PC 2\": -0.21615460662653635}, {\"PC 1\": 2.1653788629533977, \"PC 2\": 0.21528028337300728}, {\"PC 1\": 1.3445942175098249, \"PC 2\": -0.7764154251768253}, {\"PC 1\": 1.585267299308644, \"PC 2\": -0.5393070538471721}, {\"PC 1\": 1.9047435782139124, \"PC 2\": 0.11881899098269853}, {\"PC 1\": 1.9492487818644983, \"PC 2\": 0.040730259427767854}, {\"PC 1\": 3.4887653796563862, \"PC 2\": 1.1715445442635646}, {\"PC 1\": 3.7946868612099705, \"PC 2\": 0.2532655709725347}, {\"PC 1\": 1.298329824567816, \"PC 2\": -0.7610139365215078}, {\"PC 1\": 2.4281672590208485, \"PC 2\": 0.3767819712541526}, {\"PC 1\": 1.1980973722749628, \"PC 2\": -0.6055789617645946}, {\"PC 1\": 3.4992654842989572, \"PC 2\": 0.4567734669640747}, {\"PC 1\": 1.387668250181556, \"PC 2\": -0.2040309865701548}, {\"PC 1\": 2.2758536493056942, \"PC 2\": 0.33338652575664224}, {\"PC 1\": 2.614193830698334, \"PC 2\": 0.5583669502788224}, {\"PC 1\": 1.2576251829251366, \"PC 2\": -0.17913699742322287}, {\"PC 1\": 1.2906696477376018, \"PC 2\": -0.11642525182937251}, {\"PC 1\": 2.1228539805083133, \"PC 2\": -0.21085488454949}, {\"PC 1\": 2.3875644023690286, \"PC 2\": 0.46251925072626066}, {\"PC 1\": 2.840960925285384, \"PC 2\": 0.37274259104902147}, {\"PC 1\": 3.232342898295875, \"PC 2\": 1.3705240359763342}, {\"PC 1\": 2.158738373133135, \"PC 2\": -0.21832553156299334}, {\"PC 1\": 1.443102604375975, \"PC 2\": -0.14380128908272616}, {\"PC 1\": 1.77964010689857, \"PC 2\": -0.5014647947060157}, {\"PC 1\": 3.0765216210206616, \"PC 2\": 0.6857644422460569}, {\"PC 1\": 2.1449868567093042, \"PC 2\": 0.13890660887731443}, {\"PC 1\": 1.904862925137132, \"PC 2\": 0.048047508231834554}, {\"PC 1\": 1.1688534694704036, \"PC 2\": -0.16450249981508994}, {\"PC 1\": 2.107653731173269, \"PC 2\": 0.3714822491771059}, {\"PC 1\": 2.3143033946295484, \"PC 2\": 0.1826088507936987}, {\"PC 1\": 1.9224508848357529, \"PC 2\": 0.40927117617869574}, {\"PC 1\": 1.4140722251737576, \"PC 2\": -0.5749250559123048}, {\"PC 1\": 2.5633227123408093, \"PC 2\": 0.27597450223983816}, {\"PC 1\": 2.4193912198049805, \"PC 2\": 0.30350393770262885}, {\"PC 1\": 1.9440170488777375, \"PC 2\": 0.18741522204601402}, {\"PC 1\": 1.5256636313138727, \"PC 2\": -0.37502084819182296}, {\"PC 1\": 1.764045935526983, \"PC 2\": 0.07851918642935748}, {\"PC 1\": 1.901629075288233, \"PC 2\": 0.11587674824796564}, {\"PC 1\": 1.3896661333194171, \"PC 2\": -0.2828867091722692}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "RQx7UvsucIrL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goal\n",
        "\n",
        "## 1) Do NOT work on the stretch goal until you feel like you have a firm grasp of eigenvectors, eigenvalues, and PCA. Prioritize self-study over the stretch goal if you are not comfortable with those topics yet.\n",
        "\n",
        "## 2) Explore further the intuition behind eigenvalues and eigenvectors by creating your very own eigenfaces:\n",
        "\n",
        "<center>![Eigenfaces](https://i.pinimg.com/236x/1c/f1/01/1cf101a9859437a5d096a04b05be06b4--faces-tattoo.jpg)</center>\n",
        "\n",
        "You don't necessarily have to use this resource, but this will get you started: \n",
        "[Eigenface Tutorial](https://sandipanweb.wordpress.com/2018/01/06/eigenfaces-and-a-simple-face-detector-with-pca-svd-in-python/)"
      ]
    }
  ]
}