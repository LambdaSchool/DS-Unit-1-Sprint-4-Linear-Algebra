{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Clustering Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvishyst/DS-Unit-1-Sprint-4-Linear-Algebra/blob/master/Copy_of_Clustering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3rVFtGLMJM",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VS3FFSFLR3a",
        "colab_type": "text"
      },
      "source": [
        "Your assignment is to use the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" from Kaggle to try and cluster types of cancer cells. \n",
        "\n",
        "It may be helpful to use PCA to reduce the dimensions of your data first in order to obtain --but then again, maybe not. I dunno, you're the data scientist, you tell me.ðŸ¤ª \n",
        "\n",
        "Here's the original dataset for your reference:\n",
        "\n",
        "<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899RK3bBn4OE",
        "colab_type": "text"
      },
      "source": [
        "## This is a supervised learning dataset\n",
        "\n",
        "(Because it has **labels** - The \"diagnosis\" column.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5R9X6hLJQ2",
        "colab_type": "code",
        "outputId": "2010453f-e999-4d2f-882e-03054029e39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # You don't necessarily have to use this\n",
        "from sklearn.cluster import KMeans # You don't necessarily have to use this\n",
        "from sklearn.preprocessing import StandardScaler # You don't necessarily have to use this\n",
        "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst']\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/Cancer_Cells.csv\",names=columns,skiprows=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0    842302         M  ...          0.4601                  0.11890\n",
              "1    842517         M  ...          0.2750                  0.08902\n",
              "2  84300903         M  ...          0.3613                  0.08758\n",
              "3  84348301         M  ...          0.6638                  0.17300\n",
              "4  84358402         M  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDDqaU-ove4",
        "colab_type": "text"
      },
      "source": [
        "## Now it's an unsupervised learning dataset\n",
        "\n",
        "(Because we've removed the diagnosis label) - Use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86MHoPJon_aC",
        "colab_type": "code",
        "outputId": "26852ad9-9d57-4e51-a85c-f9d474439567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "target = df['diagnosis']\n",
        "df = df.drop(['diagnosis','id'], axis=1)#removing id as it is not required for clustering\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0lAq-Rtdj7m",
        "colab_type": "code",
        "outputId": "75f7becd-189b-4d4d-b86f-6319e850d2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "count   569.000000    569.000000  ...      569.000000               569.000000\n",
              "mean     14.127292     19.289649  ...        0.290076                 0.083946\n",
              "std       3.524049      4.301036  ...        0.061867                 0.018061\n",
              "min       6.981000      9.710000  ...        0.156500                 0.055040\n",
              "25%      11.700000     16.170000  ...        0.250400                 0.071460\n",
              "50%      13.370000     18.840000  ...        0.282200                 0.080040\n",
              "75%      15.780000     21.800000  ...        0.317900                 0.092080\n",
              "max      28.110000     39.280000  ...        0.663800                 0.207500\n",
              "\n",
              "[8 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud3kXTexk4ae",
        "colab_type": "code",
        "outputId": "a2c80762-177d-4bd7-93fc-033dc8a88a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Plotting elbow graph to decide on number of clusters\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(df.iloc[:,1:])\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n",
        "#This elbow at 2 indicates that taking clusters 2 for Kmeans was the correct decision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7gHgW3fELn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As this is a breast cancer dataset, setting clusters to 2, either cancer present or not present.\n",
        "kmeans = KMeans(n_clusters=2,random_state=2) #adding random state for reproducability\n",
        "#Now doing pca on data without standardisation and again pca after standardisation\n",
        "pca = PCA(n_components=30) #Same as the number of features\n",
        "#Now standardising the data before implementing the pca\n",
        "scaler = StandardScaler()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0jSXUw24CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#running standardisation on data\n",
        "df_std = scaler.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--CO7_oI6o4p",
        "colab_type": "code",
        "outputId": "e203f5d0-56be-4c69-8d06-3129a716a1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#running pca on regular data\n",
        "pca_df = pca.fit_transform(df)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First three pca components account for maximum variance"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.82044672e-01 1.61764899e-02 1.55751075e-03 1.20931964e-04\n",
            " 8.82724536e-05 6.64883951e-06 4.01713682e-06 8.22017197e-07\n",
            " 3.44135279e-07 1.86018721e-07 6.99473205e-08 1.65908880e-08\n",
            " 6.99641650e-09 4.78318306e-09 2.93549214e-09 1.41684927e-09\n",
            " 8.29577731e-10 5.20405883e-10 4.08463983e-10 3.63313378e-10\n",
            " 1.72849737e-10 1.27487508e-10 7.72682973e-11 6.28357718e-11\n",
            " 3.57302295e-11 2.76396041e-11 8.14452259e-12 6.30211541e-12\n",
            " 4.43666945e-12 1.55344680e-12]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98204467, 0.99822116, 0.99977867, 0.9998996 , 0.99998788,\n",
              "       0.99999453, 0.99999854, 0.99999936, 0.99999971, 0.99999989,\n",
              "       0.99999996, 0.99999998, 0.99999999, 0.99999999, 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiNas9m0_zcK",
        "colab_type": "code",
        "outputId": "c6e2578a-6513-48b5-a9e1-2388b2bafc49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#running pca on standardised data\n",
        "pca_df_std = pca.fit_transform(df_std)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First 10 pca components account for maximum variance"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.42720256e-01 1.89711820e-01 9.39316326e-02 6.60213492e-02\n",
            " 5.49576849e-02 4.02452204e-02 2.25073371e-02 1.58872380e-02\n",
            " 1.38964937e-02 1.16897819e-02 9.79718988e-03 8.70537901e-03\n",
            " 8.04524987e-03 5.23365745e-03 3.13783217e-03 2.66209337e-03\n",
            " 1.97996793e-03 1.75395945e-03 1.64925306e-03 1.03864675e-03\n",
            " 9.99096464e-04 9.14646751e-04 8.11361259e-04 6.01833567e-04\n",
            " 5.16042379e-04 2.72587995e-04 2.30015463e-04 5.29779290e-05\n",
            " 2.49601032e-05 4.43482743e-06]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44272026, 0.63243208, 0.72636371, 0.79238506, 0.84734274,\n",
              "       0.88758796, 0.9100953 , 0.92598254, 0.93987903, 0.95156881,\n",
              "       0.961366  , 0.97007138, 0.97811663, 0.98335029, 0.98648812,\n",
              "       0.98915022, 0.99113018, 0.99288414, 0.9945334 , 0.99557204,\n",
              "       0.99657114, 0.99748579, 0.99829715, 0.99889898, 0.99941502,\n",
              "       0.99968761, 0.99991763, 0.99997061, 0.99999557, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaI2O6wBsy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now running kmeans on pca transformation data as per maximum contained variance\n",
        "kmeans_nopca = kmeans.fit_predict(df) #without pca implemented on data\n",
        "pca_kmeans_nostd = kmeans.fit_predict(pca_df[:,:3]) #top 3 pca components cover the maximum variance\n",
        "pca_kmeans_std = kmeans.fit_predict(pca_df_std[:,:10]) #top 10 pca components cover the maximum variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1cDodIkEamt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the results and creating a dataframe\n",
        "kmeans_results = pd.DataFrame({'diagnosis':target,'kmeans_nopca':kmeans_nopca,'pca_kmeans_nostd':pca_kmeans_nostd,'pca_kmeans_std':pca_kmeans_std})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6S3jQAhge8A",
        "colab_type": "code",
        "outputId": "4e450191-c0a7-4e4d-a5df-3b20884c9413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "kmeans_results.tail()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>kmeans_nopca</th>\n",
              "      <th>pca_kmeans_nostd</th>\n",
              "      <th>pca_kmeans_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    diagnosis  kmeans_nopca  pca_kmeans_nostd  pca_kmeans_std\n",
              "564         M             0                 0               0\n",
              "565         M             0                 0               0\n",
              "566         M             1                 1               0\n",
              "567         M             0                 0               0\n",
              "568         B             1                 1               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFBDOF5TiCMn",
        "colab_type": "code",
        "outputId": "d09f4e36-a054-4670-9053-dde311a3c971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "kmeans_results.diagnosis.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXGOSzUOrbpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans_results['int_diagnosis'] = kmeans_results['diagnosis'].replace({'B':1,'M':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYEKpQYLsfeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_columns = ['kmeans_nopca', 'pca_kmeans_nostd', 'pca_kmeans_std', 'int_diagnosis']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l0LYOb3df-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "92329fb6-2a72-46ef-e212-e1ab5301f766"
      },
      "source": [
        "#Now to check for accuracy\n",
        "#For this we will use accuracy score from sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Accuracy for Kmeans with no PCA is:\",accuracy_score(kmeans_results['int_diagnosis'],kmeans_results['kmeans_nopca']))\n",
        "print(\"Accuracy for Kmeans with PCA but no stardisation is:\",accuracy_score(kmeans_results['int_diagnosis'],kmeans_results['pca_kmeans_nostd']))\n",
        "print(\"Accuracy for Kmeans with PCA and stardisation is:\",accuracy_score(kmeans_results['int_diagnosis'],kmeans_results['pca_kmeans_std']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Kmeans with no PCA is: 0.8541300527240774\n",
            "Accuracy for Kmeans with PCA but no stardisation is: 0.8541300527240774\n",
            "Accuracy for Kmeans with PCA and stardisation is: 0.9103690685413005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIpp3TkaJyRR",
        "colab_type": "text"
      },
      "source": [
        "**Based on the above results. The kmeans has a good true positive rate for all three types of transformation. But it has a high false positive for data without pca and data with pca but no standardisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskC80k3OKMA",
        "colab_type": "text"
      },
      "source": [
        "# You take it from here!\n",
        "\n",
        "See what you can come up with. You have all the know-how! \n",
        "\n",
        "- You might want to do some data exploration to see if you can find specific columns that will help you find distinct clusters of cells\n",
        "- You might want to do PCA on this data to see if that helps you find distinct linearly-separable clusters.\n",
        "  - (In the real world, truly linearly-separable clusters are rare.)\n",
        "- You might want to use an elbow chart to decide on the number of clusters to use.\n",
        "- You might want to use a scree plot to decide how many principal components to include in your clustering.\n",
        "- You might want to standardize your data before PCA (If you decide to use PCA). \n",
        "\n",
        "## Manage your time and don't spend it all on data exploration or something like that. You got this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1AeAK8PNah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBwVaGOOYsq",
        "colab_type": "text"
      },
      "source": [
        "# Stretch Goal:\n",
        "\n",
        "Once you are satisfied with your clustering, go back and add back in the labels from the original dataset to check how accurate your clustering was. Remember that this will not be a possibility in true unsupervised learning, but it might be a helpful for your learning to be able to check your work against the \"ground truth\". Try different approaches and see which one is the most successful and try understand why that might be the case. If you go back and try different methods don't ever include the actual \"diagnosis\" labels in your clustering or PCA.\n",
        "\n",
        "**Side Note** Data Science is never DONE. You just reach a point where the cost isn't worth the benefit anymore. There's always more moderate to small improvements that we could make. Don't be a perfectionist, be a pragmatist."
      ]
    }
  ]
}