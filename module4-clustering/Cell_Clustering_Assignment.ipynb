{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3rVFtGLMJM",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VS3FFSFLR3a",
        "colab_type": "text"
      },
      "source": [
        "Your assignment is to use the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" from Kaggle to try and cluster types of cancer cells. \n",
        "\n",
        "It may be helpful to use PCA to reduce the dimensions of your data first in order to obtain --but then again, maybe not. I dunno, you're the data scientist, you tell me.ðŸ¤ª \n",
        "\n",
        "Here's the original dataset for your reference:\n",
        "\n",
        "<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899RK3bBn4OE",
        "colab_type": "text"
      },
      "source": [
        "## This is a supervised learning dataset\n",
        "\n",
        "(Because it has **labels** - The \"diagnosis\" column.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5R9X6hLJQ2",
        "colab_type": "code",
        "outputId": "306d9bd1-51ab-460e-d514-c0324778000f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # You don't necessarily have to use this\n",
        "from sklearn.cluster import KMeans # You don't necessarily have to use this\n",
        "from sklearn.preprocessing import StandardScaler # You don't necessarily have to use this\n",
        "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst']\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/Cancer_Cells.csv\",names=columns,skiprows=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0    842302         M  ...          0.4601                  0.11890\n",
              "1    842517         M  ...          0.2750                  0.08902\n",
              "2  84300903         M  ...          0.3613                  0.08758\n",
              "3  84348301         M  ...          0.6638                  0.17300\n",
              "4  84358402         M  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDDqaU-ove4",
        "colab_type": "text"
      },
      "source": [
        "## Now it's an unsupervised learning dataset\n",
        "\n",
        "(Because we've removed the diagnosis label) - Use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86MHoPJon_aC",
        "colab_type": "code",
        "outputId": "710fec75-756d-4fe5-bf79-7667fb7aa386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "target = df['diagnosis']\n",
        "df = df.drop(['diagnosis','id'], axis=1)#removing id as it is not required for clustering\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0lAq-Rtdj7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "e95c75c1-8028-476b-bf83-fa39068e0208"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "count   569.000000    569.000000  ...      569.000000               569.000000\n",
              "mean     14.127292     19.289649  ...        0.290076                 0.083946\n",
              "std       3.524049      4.301036  ...        0.061867                 0.018061\n",
              "min       6.981000      9.710000  ...        0.156500                 0.055040\n",
              "25%      11.700000     16.170000  ...        0.250400                 0.071460\n",
              "50%      13.370000     18.840000  ...        0.282200                 0.080040\n",
              "75%      15.780000     21.800000  ...        0.317900                 0.092080\n",
              "max      28.110000     39.280000  ...        0.663800                 0.207500\n",
              "\n",
              "[8 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud3kXTexk4ae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f4f0f080-fa13-49f9-a30b-cd8248418a76"
      },
      "source": [
        "#Plotting elbow graph to decide on number of clusters\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(df.iloc[:,1:])\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n",
        "#This elbow at 2 indicates that taking clusters 2 for Kmeans was the correct decision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7gHgW3fELn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As this is a breast cancer dataset, setting clusters to 2, either cancer present or not present.\n",
        "kmeans = KMeans(n_clusters=2,random_state=42) #adding random state for reproducability\n",
        "#Now doing pca on data without standardisation and again pca after standardisation\n",
        "pca = PCA(n_components=30) #Same as the number of features\n",
        "#Now standardising the data before implementing the pca\n",
        "scaler = StandardScaler()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0jSXUw24CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#running standardisation on data\n",
        "df_std = scaler.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--CO7_oI6o4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cf1ab33b-cd71-4e07-b359-926087655d13"
      },
      "source": [
        "#running pca on regular data\n",
        "pca_df = pca.fit_transform(df)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First three pca components account for maximum variance"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.82044672e-01 1.61764899e-02 1.55751075e-03 1.20931964e-04\n",
            " 8.82724536e-05 6.64883951e-06 4.01713682e-06 8.22017197e-07\n",
            " 3.44135279e-07 1.86018721e-07 6.99473205e-08 1.65908880e-08\n",
            " 6.99641650e-09 4.78318306e-09 2.93549214e-09 1.41684927e-09\n",
            " 8.29577731e-10 5.20405883e-10 4.08463983e-10 3.63313378e-10\n",
            " 1.72849737e-10 1.27487508e-10 7.72682973e-11 6.28357718e-11\n",
            " 3.57302295e-11 2.76396041e-11 8.14452259e-12 6.30211541e-12\n",
            " 4.43666945e-12 1.55344680e-12]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98204467, 0.99822116, 0.99977867, 0.9998996 , 0.99998788,\n",
              "       0.99999453, 0.99999854, 0.99999936, 0.99999971, 0.99999989,\n",
              "       0.99999996, 0.99999998, 0.99999999, 0.99999999, 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiNas9m0_zcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d169e403-c95a-4a87-cd43-aa4b92fa2848"
      },
      "source": [
        "#running pca on standardised data\n",
        "pca_df_std = pca.fit_transform(df_std)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First 10 pca components account for maximum variance"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.42720256e-01 1.89711820e-01 9.39316326e-02 6.60213492e-02\n",
            " 5.49576849e-02 4.02452204e-02 2.25073371e-02 1.58872380e-02\n",
            " 1.38964937e-02 1.16897819e-02 9.79718988e-03 8.70537901e-03\n",
            " 8.04524987e-03 5.23365745e-03 3.13783217e-03 2.66209337e-03\n",
            " 1.97996793e-03 1.75395945e-03 1.64925306e-03 1.03864675e-03\n",
            " 9.99096464e-04 9.14646751e-04 8.11361259e-04 6.01833567e-04\n",
            " 5.16042379e-04 2.72587995e-04 2.30015463e-04 5.29779290e-05\n",
            " 2.49601032e-05 4.43482743e-06]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44272026, 0.63243208, 0.72636371, 0.79238506, 0.84734274,\n",
              "       0.88758796, 0.9100953 , 0.92598254, 0.93987903, 0.95156881,\n",
              "       0.961366  , 0.97007138, 0.97811663, 0.98335029, 0.98648812,\n",
              "       0.98915022, 0.99113018, 0.99288414, 0.9945334 , 0.99557204,\n",
              "       0.99657114, 0.99748579, 0.99829715, 0.99889898, 0.99941502,\n",
              "       0.99968761, 0.99991763, 0.99997061, 0.99999557, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CsMbLXQD3xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9acb3c5c-b503-47f3-ec87-719498040518"
      },
      "source": [
        "pca_df[:,:3].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaI2O6wBsy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now running kmeans on pca transformation data as per maximum contained variance\n",
        "kmeans_nopca = kmeans.fit_predict(df) #without pca implemented on data\n",
        "pca_kmeans_nostd = kmeans.fit_predict(pca_df[:,:3]) #top 3 pca components cover the maximum variance\n",
        "pca_kmeans_std = kmeans.fit_predict(pca_df_std[:,:10]) #top 10 pca components cover the maximum variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1cDodIkEamt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the results and creating a dataframe\n",
        "kmeans_results = pd.DataFrame({'diagnosis':target,'kmeans_nopca':kmeans_nopca,'pca_kmeans_nostd':pca_kmeans_nostd,'pca_kmeans_std':pca_kmeans_std})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6S3jQAhge8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "98eeaa24-9400-4932-de8f-ae5f0998216e"
      },
      "source": [
        "kmeans_results.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>kmeans_nopca</th>\n",
              "      <th>pca_kmeans_nostd</th>\n",
              "      <th>pca_kmeans_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  diagnosis  kmeans_nopca  pca_kmeans_nostd  pca_kmeans_std\n",
              "0         M             1                 1               0\n",
              "1         M             1                 1               0\n",
              "2         M             1                 1               0\n",
              "3         M             0                 0               0\n",
              "4         M             1                 1               0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFBDOF5TiCMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73b603fd-72d8-4f73-be8b-bd988358136d"
      },
      "source": [
        "kmeans_results.diagnosis.value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXGOSzUOrbpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans_results['int_diagnosis'] = kmeans_results['diagnosis'].replace({'B':1,'M':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYEKpQYLsfeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_columns = ['kmeans_nopca', 'pca_kmeans_nostd', 'pca_kmeans_std', 'int_diagnosis']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnSWwi6JtF15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a553b74b-6b1e-4333-b76c-542797834f87"
      },
      "source": [
        "kmeans_results[result_columns].sum()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca        131\n",
              "pca_kmeans_nostd    131\n",
              "pca_kmeans_std      380\n",
              "int_diagnosis       357\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588xI_r8sOR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ede444c2-87f1-4e4f-cf2c-fac5c54bef11"
      },
      "source": [
        "kmeans_results[kmeans_results.diagnosis=='M'][result_columns].sum()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca        130\n",
              "pca_kmeans_nostd    130\n",
              "pca_kmeans_std       37\n",
              "int_diagnosis         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO8FOo7XtCGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "01e8ee8e-bf9b-4960-b27a-b39d81a8698f"
      },
      "source": [
        "kmeans_results[kmeans_results.diagnosis=='B'][result_columns].sum()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca          1\n",
              "pca_kmeans_nostd      1\n",
              "pca_kmeans_std      343\n",
              "int_diagnosis       357\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskC80k3OKMA",
        "colab_type": "text"
      },
      "source": [
        "# You take it from here!\n",
        "\n",
        "See what you can come up with. You have all the know-how! \n",
        "\n",
        "- You might want to do some data exploration to see if you can find specific columns that will help you find distinct clusters of cells\n",
        "- You might want to do PCA on this data to see if that helps you find distinct linearly-separable clusters.\n",
        "  - (In the real world, truly linearly-separable clusters are rare.)\n",
        "- You might want to use an elbow chart to decide on the number of clusters to use.\n",
        "- You might want to use a scree plot to decide how many principal components to include in your clustering.\n",
        "- You might want to standardize your data before PCA (If you decide to use PCA). \n",
        "\n",
        "## Manage your time and don't spend it all on data exploration or something like that. You got this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1AeAK8PNah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBwVaGOOYsq",
        "colab_type": "text"
      },
      "source": [
        "# Stretch Goal:\n",
        "\n",
        "Once you are satisfied with your clustering, go back and add back in the labels from the original dataset to check how accurate your clustering was. Remember that this will not be a possibility in true unsupervised learning, but it might be a helpful for your learning to be able to check your work against the \"ground truth\". Try different approaches and see which one is the most successful and try understand why that might be the case. If you go back and try different methods don't ever include the actual \"diagnosis\" labels in your clustering or PCA.\n",
        "\n",
        "**Side Note** Data Science is never DONE. You just reach a point where the cost isn't worth the benefit anymore. There's always more moderate to small improvements that we could make. Don't be a perfectionist, be a pragmatist."
      ]
    }
  ]
}