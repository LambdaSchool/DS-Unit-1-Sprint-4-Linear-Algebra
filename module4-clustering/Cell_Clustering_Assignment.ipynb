{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3rVFtGLMJM",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VS3FFSFLR3a",
        "colab_type": "text"
      },
      "source": [
        "Your assignment is to use the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" from Kaggle to try and cluster types of cancer cells. \n",
        "\n",
        "It may be helpful to use PCA to reduce the dimensions of your data first in order to obtain --but then again, maybe not. I dunno, you're the data scientist, you tell me.ðŸ¤ª \n",
        "\n",
        "Here's the original dataset for your reference:\n",
        "\n",
        "<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899RK3bBn4OE",
        "colab_type": "text"
      },
      "source": [
        "## This is a supervised learning dataset\n",
        "\n",
        "(Because it has **labels** - The \"diagnosis\" column.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5R9X6hLJQ2",
        "colab_type": "code",
        "outputId": "ab87c391-14de-4b17-875d-79d02d242da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # You don't necessarily have to use this\n",
        "from sklearn.cluster import KMeans # You don't necessarily have to use this\n",
        "from sklearn.preprocessing import StandardScaler # You don't necessarily have to use this\n",
        "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst']\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/Cancer_Cells.csv\",names=columns,skiprows=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0    842302         M  ...          0.4601                  0.11890\n",
              "1    842517         M  ...          0.2750                  0.08902\n",
              "2  84300903         M  ...          0.3613                  0.08758\n",
              "3  84348301         M  ...          0.6638                  0.17300\n",
              "4  84358402         M  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDDqaU-ove4",
        "colab_type": "text"
      },
      "source": [
        "## Now it's an unsupervised learning dataset\n",
        "\n",
        "(Because we've removed the diagnosis label) - Use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86MHoPJon_aC",
        "colab_type": "code",
        "outputId": "9f98cd2f-3a43-4623-a61f-b6d4a683c743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "target = df['diagnosis']\n",
        "df = df.drop(['diagnosis','id'], axis=1)#removing id as it is not required for clustering\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0lAq-Rtdj7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "5c8d6c32-22a8-41bc-91d7-75bb95d8c252"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "count   569.000000    569.000000  ...      569.000000               569.000000\n",
              "mean     14.127292     19.289649  ...        0.290076                 0.083946\n",
              "std       3.524049      4.301036  ...        0.061867                 0.018061\n",
              "min       6.981000      9.710000  ...        0.156500                 0.055040\n",
              "25%      11.700000     16.170000  ...        0.250400                 0.071460\n",
              "50%      13.370000     18.840000  ...        0.282200                 0.080040\n",
              "75%      15.780000     21.800000  ...        0.317900                 0.092080\n",
              "max      28.110000     39.280000  ...        0.663800                 0.207500\n",
              "\n",
              "[8 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud3kXTexk4ae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c790a208-18a8-4727-b2fe-5a1addeccc59"
      },
      "source": [
        "#Plotting elbow graph to decide on number of clusters\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(df.iloc[:,1:])\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n",
        "#This elbow at 2 indicates that taking clusters 2 for Kmeans was the correct decision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xu8XPPZ///XOyHUIRKyCRKCutXe\nSpG0lKo4VRVpS5VWVUv92rstdTvc1dPduu+2SkurDi2lcapDnUsojThEtT9JikpQRCMhIeKQIES4\nvn981tizd/ZhVjKz156Z9/PxWI9Zp1lzzSR7rvmsz7U+SxGBmZlZyYCiAzAzs/7FicHMzDpwYjAz\nsw6cGMzMrAMnBjMz68CJwczMOnBisB5JOlzS5LLlkPTeImOqlmq+F0n/lrRHNY7VH0h6VdKmNThu\nh/9PnbaNyv5NVqr261o+TgxW+lJbnH0ZlKazio4L3v0iCUlndFo/Lls/vsLj3CnpyJoE2ftrj5e0\npNPn+9kqHn9fSf+/pNckLZB0maQROZ6/zGcTEWtExMxqxWj1xYnBSvbLvgxK0zeKDqjMk8BBnX5J\nfhH4V0HxLI9TO32+V+Y9gKSBXaw7EPgD8EtgGNAGvAlMljR0RYO25uTEYMtjH0kzJb0g6TRJAwAk\nDZD0PUmzJD0v6WJJa2XbLpJ0XDa/YfZr/+vZ8maSXiwdpwvzgH8CH8v2Xxv4MHBj+U6SdpD0V0kv\nS3pQ0q7Z+h8DHwHO6qI1tIekx7PnnC1Jvb2XbPsXsm0LJH13eT9ISVtmv9hfljRd0v5l28ZLOlfS\nBEmvAWM7PVfAL4D/i4g/RMTiiJgHHAm8Chyb7Xe4pHslnSXpFUmPStq9p8+m/DRbFsc5km7J9rlX\n0nBJv5T0Una8bcvi+rakJyUtkjRD0qeW87M5IGvNbrU8z7fl58Rgy+NTwGhgO2Ac8OVs/eHZNBbY\nFFgDKH0J3wXsms1/FJgJ7FK2fE9EvNPDa14MHJbNHwzcQPplDKRkA9wM/B+wNnA8cI2kloj4LnAP\n8I0uWkP7AmOArYGDyJJPT+9FUitwLvAFYANgHaDiUzdlMa8M/Am4DVgX+CZwmaQtynb7HPBjYE2g\n87n5LYCNgD+Wr8w+x2uAPctWf4jU8hoG/A9wraS1e/lsyh0EfC97/pvAfcC0bPlq4PSyfZ8kJZu1\ngB8Bl0pav8cPoxNJXwJ+BuwREQ/nea6tuLpNDJIuzH7J9fqfRtJGkiZJ+oekhyTt0xcx1pnrs1+t\npekrPez7s4h4MSKeJp3COCRb/3ng9IiYGRGvAicBB2engO4Cds5aBbsApwI7Zc/7aLa9J9cBu2a/\n2g8jJYpyhwITImJCRLwTEbcDU4De/q1PiYiXs/cyCfhABe/lQOCmiLg7It4Evg/0lNQAji/7bF/I\n1u1ASjinRMSSiLgDuIn2zxPghoi4N3tPb3Q65rDscW4Xrze3bDvA88AvI+Kt7DTWY8Aneom53HUR\nMTWL4TrgjYi4OCLeBq4E3m0xRMQfI+LZLOYrgceBD+Z4rW8BJwC7RsQTOZ5nVVK3iQEYD+xd4b7f\nA66KiG1JvzbPqVVQdeyTETGkbDq/h31nl83PIv1qJnuc1WnbSsB6EfEk8Brpi/cjpC/AZ7Nfx70m\nhohYTGoRfA9YJyLu7bTLxsBnypMbsDPQ2y/VeWXzr5O+qHt8L9m2dz+DiHgNWNDL6/y87LMtfWFv\nAMzu1FKaBWxYtlz+WXdWSjBdvcf1y7YDPBMdR8ws/3erxHNl84u7WC59bkg6TNIDZf8OW9ExSfXm\nBODsiJiT4zlWRXWbGCLibuDF8nXZuepbJU2VdI+k95V2BwZn82sBz/ZhqI1oZNn8RrR/ns+SvqDL\nty2l/UvkLtKv7UER8Uy2/EVgKPBABa97MXAccGkX22YDl3RKbqtHxCnZ9rzDCPf0XuZS9hlIWo10\nOimvZ4GRnfpWNgKeKVvuKe7HgDnAZ8pXZsc7AJhYtnrDUv9J2euU/t2qNsSypI2B84FvkBL4EOBh\nQD0+saO9gO9JOqBacVk+dZsYunEe8M2I2J50jrnUMvghcKikOcAE0rlcW34nSBoqaSRwDOlUAsDl\nwLGSNpG0BvAT4MqIWJptv4v0hXF3tnxntjw5OyXRm7tI581/3cW2S4H9JH1M0kBJq0raVe1lm8+R\n+goq1dN7uRrYV9LOkgYBJ7N8f0t/J7VSTpS0ctZZvh9wRSVPzloAx5O+RD+XvefhwO9IP4TKS3zX\nBY7OXuczwJakvwXI/9n0ZHVSopkP7/YV5O08nk46G3B2eWe89Z2GSQzZH++HgT9KegD4Le1N7EOA\n8RExgnTO+RJ1XwHTrP6kjnX21/Ww7w3AVNKv/JuBC7L1FwKXkL74nwLeoGMSvovUiVpKDJOB1cqW\nexTJxIh4sYtts0kd4d8hfSnNJp2SKP07/wo4MKuiObOCl+v2vUTEdODrpDLRucBLpF/uuUTEElIi\n+DjptM85wGER8WiOY1xJ6gQ/lnQ6awbwHmCniCg/vfV3YPPsdX4MHFi2Pe9n01M8M0iVUveREs77\ngc6n/So5zoOkwoDzJX18RWKy/FTPN+qRNIrUCbiVpMHAYxGxzPlWSdOBvbMvDyTNBHaIiOf7Ml6z\nIkg6HDgyInYuOharDw3zqzkiFgJPZc1klGyTbX4aKNVtbwmsStbUNTOzjuo2MUi6nNRc3ULSHElH\nkEoMj5D0IOk85bhs9+OAr2TrLwcOj3puKpmZ1VBdn0oyM7Pqq9sWg5mZ1UZdDm87bNiwGDVqVNFh\nmJnVlalTp74QES297VeXiWHUqFFMmTKl6DDMzOqKpFm97+VTSWZm1okTg5mZdeDEYGZmHTgxmJlZ\nB04MZmbWQVMkhlNPhUmTOq6bNCmtNzOzjpoiMYwZAwcd1J4cJk1Ky2PGFBuXmVl/VJfXMeQ1dixc\ndRV8+tOw5Zbw+ONpeezY3p9rZtZsmqLFACkJ7LQT3HcfHHKIk4KZWXeaJjFMmgT3ZrcLueiiZfsc\nzMwsqWlikDRS0iRJMyRNl3RMF/vsKumV7ObhD0j6QbXjKPUpXHRRWj744I59DmZm1q7WfQxLgeMi\nYpqkNYGpkm7Pbv9X7p6I2LdWQdx/f3ufwvrrw5tvpuX77/cpJTOzzmqaGCJiLumeuETEIkmPABuS\n7kvbZ048sX2+rQ2mT08JwUnBzGxZfdbHkN2feVvSTck721HSg5JukdTWzfOPkjRF0pT585f/rpyt\nrfDII/DOO8t9CDOzhtYniUHSGsA1wLeyezOXmwZsHBHbAL8Gru/qGBFxXkSMjojRLS29DiferbY2\neO01ePrp5T6EmVlDq3likLQyKSlcFhHXdt4eEQsj4tVsfgKwsqRhtYqntTU9Tp9eq1cwM6tvta5K\nEnAB8EhEnN7NPsOz/ZD0wSymBbWKqS07UTWjT3s5zMzqR62rknYCvgD8U9ID2brvABsBRMRvgAOB\nr0laCiwGDo6IqFVAQ4emyiS3GMzMulbrqqTJgHrZ5yzgrFrG0Vlrq1sMZmbdaZorn8u1taXE4Mok\nM7NlNWViaG1NlUmzZxcdiZlZ/9OUiaHUAe1+BjOzZTVlYnDJqplZ95oyMay9Ngwf7g5oM7OuNGVi\ngPYxk8zMrKOmTQylktXaXTFhZlafmjYxeMwkM7OuNW1iKHVAu5/BzKyjpk0MLlk1M+ta0yaGUmWS\nE4OZWUdNmxjAYyaZmXWlqRNDacwkVyaZmbVr6sTQ2gqvvuoxk8zMyjV1YnAHtJnZspo6Mbhk1cxs\nWU2dGNZZB9Zbzy0GM7NyTZ0YwGMmmZl11vSJwWMmmZl11PSJoa3NlUlmZuWaPjG4A9rMrKOmTwwu\nWTUz66jpE8M668C667rFYGZW0vSJAVyZZGZWruLEIOkYSYOVXCBpmqS9ahlcX/GYSWZm7fK0GL4c\nEQuBvYChwBeAU2oSVR9rbYVFi2DOnKIjMTMrXp7EoOxxH+CSiJhetq6uuQPazKxdnsQwVdJtpMTw\nZ0lrAu/UJqy+5ZJVM7N2K+XY9wjgA8DMiHhd0jrAl2oTVt8aNixVJrnFYGaWr8UQQCtwdLa8OrBq\n1SMqiCuTzMySPInhHGBH4JBseRFwdtUjKojHTDIzS/Ikhg9FxNeBNwAi4iVgUE9PkDRS0iRJMyRN\nl3RMF/tI0pmSnpD0kKTtcr2DKmlrc2WSmRnkSwxvSRpIOqWEpBZ673xeChwXEa3ADsDXJbV22ufj\nwObZdBRwbo6YqsYd0GZmSZ7EcCZwHbCupB8Dk4Gf9PSEiJgbEdOy+UXAI8CGnXYbB1wcyd+AIZLW\nzxFXVbhk1cwsqbgqKSIukzQV2J10/cInI+KRSp8vaRSwLfD3Tps2BMoHvZ6TrZvb6flHkVoUbLTR\nRpW+bMWGDYOWFrcYzMzyDImxA/BMRJwdEWcBz0j6UIXPXQO4BvhWdvV0bhFxXkSMjojRLS0ty3OI\nXrkyycws36mkc4FXy5ZfpYL+AEkrk5LCZRFxbRe7PAOMLFseka3rcx4zycws55AYEe1fmRHxDr2c\nipIk4ALgkYg4vZvdbgQOy6qTdgBeiYi53exbU62tsHAhPFNIWjIz6x/yXPk8U9LRtLcS/hOY2ctz\ndiINtvdPSQ9k674DbAQQEb8BJpCG2XgCeJ0Cr6Yu74AeMaKoKMzMipUnMXyVVJn0PVLJ6kSyzuDu\nRMRkehloL2uFfD1HHDVTXrL6sY8VG4uZWVHyVCU9Dxxcw1gK19KSJndAm1kzqzgxZBe0fQUYVf68\niPhy9cMqTmloDDOzZpXnVNINwD3AX4C3axNO8dra4NJLU2WSGuJuE2Zm+eRJDKtFxH/XLJJ+oq2t\nvTLJHdBm1ozylKveJGmfmkXST3jMJDNrdnkSwzGk5LBY0kJJiyQt11XM/ZnHTDKzZpenKmnNWgbS\nX7S0pHGT3GIws2aVp48BSUNJw2O/e+e2iLi72kEVzWMmmVkzyzOI3pHA3cCfgR9ljz+sTVjF8t3c\nzKyZ5e1jGAPMioixpCG0X65JVAVra4NXXoFnny06EjOzvpcnMbwREW8ASFolIh4FtqhNWMVyB7SZ\nNbM8iWGOpCHA9cDtkm4AZtUmrGK5ZNXMmlmeqqRPZbM/lDQJWAu4pSZRFWzddVNlklsMZtaM8nQ+\nX1Kaj4i7IuJG4MKaRNUPeMwkM2tWeU4ltZUvSBoIbF/dcPqPUsmqK5PMrNn0mhgknSRpEbB1dsXz\nwmz5edLAeg2ptTVVJs0t5F5yZmbF6TUxRMRPs6ueT4uIwdm0ZkSsExEn9UGMhXBlkpk1q7yD6K0O\nIOlQSadL2rhGcRXOicHMmlWexHAu8LqkbYDjgCeBi2sSVT/Q0gLrrOMOaDNrPnkSw9Ls/szjgLMi\n4mygYQfWkzxmkpk1pzyJYZGkk4BDgZslDQBWrk1Y/YPHTDKzZpQnMXwWeBM4IiLmASOA02oSVT/R\n1gYvv+zKJDNrLnmufJ4HnF62/DQN3McA7R3QM2bABhsUG4uZWV+p5DqGydnjorLrGBr2Dm7lSmMm\nuZ/BzJpJry2GiNg5e2zYjuburLtuqkxyYjCzZtJrYpC0dk/bI+LF6oXTv0geM8nMmk8lfQxTgQAE\nbAS8lM0PAZ4GNqlZdP1AWxtccUWqTJKKjsbMrPYqGRJjk4jYFPgLsF9EDIuIdYB9gdtqHWDRWltT\nZdK8eUVHYmbWN/KUq+4QERNKCxFxC/Dh6ofUv3hoDDNrNnkSw7OSvidpVDZ9F2j4uyKXl6yamTWD\nPInhEKAFuA64Nps/pKcnSLpQ0vOSHu5m+66SXpH0QDb9IEc8fWLddWHttd1iMLPmkecCtxeBY7rb\nLunXEfHNTqvHA2fR84Vw90TEvpXG0dc8ZpKZNZs8LYbe7NR5RUTcDdR9OavHTDKzZlLNxLC8dpT0\noKRbJLV1t5OkoyRNkTRl/vz5fRkfbW3w0kuuTDKz5lB0YpgGbBwR2wC/Bq7vbseIOC8iRkfE6JaW\nlj4LENqHxnAHtJk1g2omhtyXf0XEwoh4NZufAKwsaVgVY6oKl6yaWTOpZmL4Vd4nSBoupeuJJX0w\ni2dBFWOqivXWc2WSmTWPSsZK+hNpSIwuRcT+2eP4Lp57ObArMEzSHOB/yG7uExG/AQ4EviZpKbAY\nODi7S1y/4jGTzKyZVFKu+vPs8dPAcODSbPkQ4LmenhgRPV7nEBFnkcpZ+722NrjqKo+ZZGaNr5Jh\nt+8CkPSLiBhdtulPkqbULLJ+prU1VSY99xwMH150NGZmtZOnj2F1SZuWFiRtAqxe/ZD6J3dAm1mz\nqPjKZ+BY4E5JM0kVSBsD/19NouqHyktWd9+92FjMzGopz5AYt0raHHhfturRiHizNmH1P8OHw9Ch\nbjGYWeOr+FSSpNWAE4BvRMSDwEaS+u0YR9XmMZPMrFnk6WP4PbAE2DFbfgb4v6pH1I+1tqbE0P8K\nas3MqidPYtgsIk4F3gKIiNdZjqud61lpzKTneizSNTOrb3kSwxJJ7yG72E3SZkDT9DGAx0wys+aQ\nJzH8D3ArMFLSZcBE4MSaRNVPuWTVzJpBRVVJ2XhGj5Kuft6BdArpmIh4oYax9TvDh8OQIW4xmFlj\nqygxRERImhAR7wdurnFM/ZYrk8ysGeQ5lTRN0piaRVInSonBlUlm1qjyXPn8IeDzkmYBr5FOJ0VE\nbF2TyPqp1lZ48UV4/vk0HLeZWaPJkxg+VrMo6kh5B7QTg5k1oopPJUXErIiYRbpvQpRNTcUlq2bW\n6PIMibG/pMeBp4C7gH8Dt9Qorn5r/fVTZZI7oM2sUeXpfP5fUqnqvyJiE2B34G81iaofK1UmucVg\nZo0qT2J4KyIWAAMkDYiIScDo3p7UiDxmkpk1sjyJ4WVJawB3A5dJ+hWpOqnptLXBggWpMsnMrNHk\nSQzjSB3Px5KGxngS2K8WQfV37oA2s0aW50Y95a2Di2oQS90oL1kdO7bYWMzMqq3ixCBpEe3lqYOA\nlYHXImJwLQLrz9ZfH9Zayy0GM2tMeVoMa5bms0H1xpGqlJqOx0wys0aWp4/hXZFcTxNfDe2SVTNr\nVHlOJX26bHEAqVT1japHVCdaW+H881Nl0rrrFh2NmVn15BkrqbwCaSnpyudxVY2mjpR3QDsxmFkj\nydPH8KVaBlJvyktWXZlkZo0kz6mkM3vaHhFHr3g49WODDVJlkjugzazR5Ol8XhXYDng8mz5AKlud\nmk1NRUqtBndAm1mjydPHsDWwc0QsBZD0G+CeiPhqTSKrA21tcP31RUdhZlZdeVoMQ4Hyi9nWyNZ1\nS9KFkp6X9HA32yXpTElPSHpI0nY54ilcWxu88ALMn190JGZm1ZMnMZwC/EPSeEkXAdOAn/TynPHA\n3j1s/ziweTYdBZybI57ClTqg3c9gZo0kzx3cfk+67/N1wLXAjhHR45hJEXE38GIPu4wDLs4umPsb\nMETS+pXGVLTyklUzs0aR5w5uOwGLIuIGYE3gREkbr+DrbwjMLluek62rCxtsAIMHuwPazBpLnlNJ\n5wKvS9oG+C/SsNsX1ySqLkg6StIUSVPm95OT+h4zycwaUZ7EsDQignT65+yIOJvUclgRzwAjy5ZH\nZOuWERHnRcToiBjd0tKygi9bPS5ZNbNGkycxLJJ0EnAocLOkAaSht1fEjcBhWXXSDsArETF3BY/Z\np9raUlVSP2nEmJmtsDyJ4bPAm8ARETGP9Ov+tJ6eIOly4D5gC0lzJB0h6auSStc+TABmAk8A5wP/\nmfcNFK3UAe1Wg5k1ijxjJc0DTi9bfpqyPgZJ90XEjp2ec0gvxwzg6xVH2w+Vl6x+9KPFxmJmVg3L\ndT+GbqxaxWPVjQ03TJVJ7oA2s0ZRzcQQve/SeDxmkpk1mmomhqblklUzayS9JgZJq1R4LK1gLHWr\ntdWVSWbWOCppMdwHIOmSXvb7woqHU59cmWRmjaSSqqRBkj4HfLjTfZ8BiIhrs8cuR1BtBuVjJrky\nyczqXSWJ4avA54EhdLzvM6QO52urHVS9KVUmucVgZo2g18QQEZOByZKmRMQFfRBT3SlVJrkD2swa\nQZ6qpEskHS3p6mz6pqQVHRKjYbhk1cwaRZ7EcA6wffZ4Dun+z3V1Y51aOfVUWGUVeP75dEc3gEmT\n0nozs3qT557PYyJim7LlOyQ9WO2A6tGYMfCT7F52M2bA22/DQQfBVVcVG5eZ2fLI02J4W9JmpQVJ\nmwJvVz+k+jN2LPz2t2n+5JPbk8LYscXGZWa2PPIkhhOASZLulHQXcAdwXG3Cqj8HHQQjRsDEibDn\nnk4KZla/8oyuOlHS5sAW2arHIuLN0nZJe0bE7dUOsF7ceScsXgzDh8Pll8N228HxxxcdlZlZfrnG\nSoqINyPioWx6s9Pmn1UxrroyaVJqMfzxj/Dww7DppnDCCXDGGUVHZmaWXzUH0WvasZLuv7+9T2Gd\ndeBvf4ONN4b//m+YPLno6MzM8vGw21Vw4okd+xRaWlJy2HRT2GefNG9mVi887HaNDB8Od9wB660H\ne+8NU6YUHZGZWWWqmRj+XcVjNYQNNkjJYe21Ya+94IEHio7IzKx3FVclSRoIfAIYVf68iDg9e1xm\n5FWDkSNTcthlF9hjj1S9tNVWRUdlZta9PC2GPwGHA+sAa5ZN1otRo1Ll0iqrwO67wyOPFB2RmVn3\n8gyJMSIitq5ZJA1us81Sy2HXXWG33eCuu+A//qPoqMzMlpWnxXCLpL1qFkkT2GKLdGX022+n5PDk\nk0VHZGa2rDyJ4W/AdZIWS1ooaZGkhbUKrFG1tqbk8MYbKTnMmlV0RGZmHeVJDKcDOwKrRcTgiFgz\nIgbXKK6G9v73w+23w8KF6fqHOXOKjsjMrF2exDAbeDgimvZCtmradlu47TZYsCC1HJ59tuiIzMyS\nPJ3PM4E7Jd0CvDtOUqlc1fIbMwZuvTVd47D77qmUdb31io7KzJpdnhbDU8BEYBAuV62aHXeECRPg\n6adTcpg/v+iIzKzZ5Rl2+0e1DKSZfeQjcNNNaVylPfdsv1razKwIea58nkQXA+VFxG5VjahJjR0L\nN9wA+++fTi395S8wZEjRUZlZM8rTx1B+25lVgQOApdUNp7nttRdcey188pNp4L3bboPBrvsysz5W\ncR9DREwtm+6NiP8Cdu3teZL2lvSYpCckfbuL7YdLmi/pgWw6Mt9baCz77JNu+DN1app/9dWiIzKz\nZlNxYpC0dtk0TNLewFq9PGcgcDbwcaAVOERSaxe7XhkRH8im3+V5A41o3Di44op0H4d994XXXy86\nIjNrJnmqkqYCU7Lpr8B/AUf08pwPAk9ExMyIWAJcAYxbnkCbzQEHwCWXwN13p87pxYvbt02aBKee\nWlxsZtbYek0MksZIGh4Rm0TEpsCPgEezaUYvT9+QdGFcyZxsXWcHSHpI0tWSRnYTx1GSpkiaMr9J\najoPOSTdHnTaNPjoR+HNN9vvLz1mTNHRmVmjqqTF8FtgCYCkXYCfAhcBrwDnVSGGPwGjspFbb8+O\nvYyIOC8iRkfE6JaWliq8bH346U/h+OPTfaW33BIOPLD9/tJmZrVQSWIYGBEvZvOfBc6LiGsi4vvA\ne3t57jNAeQtgRLbuXRGxICJKV1L/Dti+gpiaymmnpb6Gp56CRYvgH/+Apa4HM7MaqSgxSCqVte4O\n3FG2rbdy1/uBzSVtImkQcDBwY/kOktYvW9wf8G1sOpk0KXVEf/ObIMFxx8H228N99xUdmZk1okoS\nw+XAXZJuABYD9wBIei/pdFK3ImIp8A3gz6Qv/KsiYrqkkyXtn+12tKTpkh4EjibdJc4ypT6Fq66C\nM8+EW25J1zY8+yx8+MNw1FHw4ou9H8fMrFKqZLBUSTsA6wO3RcRr2br/ANaIiGm1DXFZo0ePjilT\npvT1yxbi1FNTR3N5n8KkSTB5chq2+4wzYOhQ+PnP4bDDUovCzKwrkqZGxOhe96vHUbSbKTH05qGH\n4KtfTaeVdtkFzj033QzIzKyzShNDnusYrB/aeuvUejj/fHj4YdhmGzjpJF8UZ2bLz4mhAQwYAEce\nCY8+CoceCqeckloNN91UdGRmVo+cGBpISwv8/vdw112w+uqw337wqU+lez2YmVXKiaEB7bJLutbh\nlFPgz39OF8b9/Ofw1ltFR2Zm9cCJoUENGpSG05gxI90Z7oQTYLvt4N57i47MzPo7J4YGN2oU3Hgj\nXH89vPIK7Lxz6o9YsKDoyMysv3JiaBLjxqXWwwknwEUXwRZbwIUXws9+lq6LKOfRW82amxNDE1lj\njfSFP20avO99cMQRaWjvAw5oTw4evdXMnBia0Pvfn+7zcMEFMHduOsX0iU/A0Ue3D7/h0VvNmpcT\nQ5MaMAC+/GV47DE4/PB0I6Bf/zptmzAB7rzTVUxmzcqJockNG5Yuihs6FPbaK7UezjgjtRiGDYPP\nfAbGj4fnnis6UjPrK70Nm20NrtSncM01KRlMmpSSwdFHw+zZqfVw9dVp3zFj0imnffZJw34P8M8K\ns4bkP+0md//9HfsUxo6FP/4RVl01jb80Z07qrP7f/4WBA+FHP4IPfhA22AC+9KWUNF7pcfB1M6s3\nHl3VcnnhBbj1Vrj55vT48suw0krp+ohPfCJN73tfGv67uyHD778fTjyxuPdg1qw8uqrVRKlP4vLL\nYf78VN103HHpgrkTTkiD9222Wbrb3DvvpNNULoU1qy9uMVjVPP106pO4+WaYODFVOg0alLbtvHNq\nKZx2Gnz2szBkSLGxmjUj36jHCrV4cRrl9eab4dJL0ymnckOHwqabtk+bbdY+P3JkOj3VE5+mMsuv\n0sTgqiSrife8B/beG1ZZBa64In1Zn38+fOtbsNpqMHNmmh54II3jVH7NxMCBsPHGyyaM0jRkSEoK\n5Rfjld8b28xWjBOD1Uz5l/XYsSlRlJaPP759v7ffhmeeSYniySfbk8bMmamM9oUXOh537bVTgthq\nq9TZvdtucM896fqL7baDCN/72mxF+FSS1Uy1TvcsXAhPPbVs0iglknfe6bj/Gmuk01EjRqTHruYH\nD+6b2M36E/cxWMMrtUg+9zm4+OJUCTV4cLr2Yvbs9sd581IrotzgwV0njtLjU0/BF7/Y9akqjyNl\n9cqJwRpa5y/qnr64lyxJgwVHJjo+AAAIoklEQVTOnt0xYZTPdzXkx+qrwxtvpGQxdy7ssQe0taVT\nWUOHtj+Wzw8e3PsV4W6NWFHc+WwNrasrtq+6Kq3vnBgGDUqd2Rtv3P3xlixJ/RzlyWLOHLjtNvjX\nv9L1G9OmwV/+kvbtzoABqXO8PFl0TiAvvACf/GS6iny33WD69NTaueKKFf9cwInHVpxbDGbdKLVC\nvvY1OPfclHh23TWV4r70UppefLHjY0/zL72UOtp7ssYasNZaqeWx1lodp87rutvnr3+tvDWVVy2T\njhNa7bnFYLYCOn+Zjh3bcXm11WDDDfMdMwIWLWpPFqefnq7x2G+/1HJ45ZX2aeHC9PjSS/Dvf7ev\ne/313l9nlVVSufCee6aWzoIF6R4cv/gFnHNOiv0970lT+Xzn5a7m3/veNMjiFVekU2vVLBOudQmy\nE0/lnBjMupDnVFWlpPSLfvDg1Ll9663w/e+n1sixx1Z23Lfeak8apcfOyaQ0TZ6cTlNtsknqL5k3\nLyWWxYvTVJpfujT/e9lzz/R+ImDNNeHzn08JadCg9Njd1Nv2z30O9t8/JZ2JE+Hb306trMmT08CO\n3U29XRAJtU08tU46fZ3UfCrJrI/l6Thf0dcoPw3W3bHfemvZZNHb/M03pyvbP/QhGD0a3nwzTUuW\ntM+XTz2tr8YNoVZaqefEUZoWLoS//x223BIefTRdWzNqFKy8csdppZV6Xu687p//hB/+EH76U9hx\nx9QfddxxcOaZaTiYlVbqOA0c2HG5t+tuqvV/xlVJZv1UrX/91Trx5Ek6lXjnnfbEMXEifOUrcMgh\n8Ic/pC/arbZKCemNN7qfettevs+zz6YW1WqrpWTx1lvtU299QLUyYEDXCaN83ZIlqXpu333h3nuX\n73N3YjBrUrVMPLVMOv2hJRWRTq2VEkX5fCXL48fDlVfCAQfAgQem7eXT228vu663beXrH3gAZsxI\npyBPPjn/+680MRARNZ2AvYHHgCeAb3exfRXgymz734FRvR1z++23DzPrez/7WcQdd3Rcd8cdaX1/\nPnbpWMOGtb9G5+VqHf/736/ucat5fGBKVPK9XclOyzsBA4EngU2BQcCDQGunff4T+E02fzBwZW/H\ndWIws7xqmXj6Kums6PErTQy1vlHPB4EnImJmRCwBrgDGddpnHHBRNn81sLvkIdDMrLpOPHHZU1Jj\nx1anX6enKrZqqPXxO6tpH4OkA4G9I+LIbPkLwIci4htl+zyc7TMnW34y2+eFTsc6CjgKYKONNtp+\n1qxZNYvbzKwRNdytPSPivIgYHRGjW1paig7HzKxh1ToxPAOMLFseka3rch9JKwFrAQtqHJeZmXWj\n1onhfmBzSZtIGkTqXL6x0z43Al/M5g8E7ohant8yM7Me1XRIjIhYKukbwJ9JFUoXRsR0SSeTesdv\nBC4ALpH0BPAiKXmYmVlBaj5WUkRMACZ0WveDsvk3gM/UOg4zM6tMXV75LGk+0F/LkoYBL/S6V/9U\nr7HXa9zg2IvSrLFvHBG9Vu/UZWLozyRNqaQcrD+q19jrNW5w7EVx7D2rm3JVMzPrG04MZmbWgRND\n9Z1XdAAroF5jr9e4wbEXxbH3wH0MZmbWgVsMZmbWgRODmZl14MRQBZJGSpokaYak6ZKOKTqmvCQN\nlPQPSTcVHUsekoZIulrSo5IekbRj0TFVStKx2f+XhyVdLmnVomPqjqQLJT2fjYZcWre2pNslPZ49\nDi0yxu50E/tp2f+ZhyRdJ2lIkTF2p6vYy7YdJykkDav26zoxVMdS4LiIaAV2AL4uqbXgmPI6Bnik\n6CCWw6+AWyPifcA21Ml7kLQhcDQwOiK2Ig0Z05+HgxlPuhtjuW8DEyNic2BittwfjWfZ2G8HtoqI\nrYF/ASf1dVAVGs+ysSNpJLAX8HQtXtSJoQoiYm5ETMvmF5G+nDYsNqrKSRoBfAL4XdGx5CFpLWAX\n0nhbRMSSiHi52KhyWQl4Tzaq8GrAswXH062IuJs0llm58ptsXQR8sk+DqlBXsUfEbRGxNFv8G2nk\n536nm88d4AzgRKAm1UNODFUmaRSwLen+1fXil6T/ZO8UHUhOmwDzgd9np8F+J2n1ooOqREQ8A/yc\n9ItvLvBKRNxWbFS5rRcRc7P5ecB6RQazAr4M3FJ0EJWSNA54JiIerNVrODFUkaQ1gGuAb0XEwqLj\nqYSkfYHnI2Jq0bEsh5WA7YBzI2Jb4DX67+mMDrLz8eNIyW0DYHVJhxYb1fLLhsqvu9p3Sd8lnQq+\nrOhYKiFpNeA7wA9623dFODFUiaSVSUnhsoi4tuh4ctgJ2F/Sv0n35N5N0qXFhlSxOcCciCi1zq4m\nJYp6sAfwVETMj4i3gGuBDxccU17PSVofIHt8vuB4cpF0OLAv8Pk6ugfMZqQfEw9mf7MjgGmShlfz\nRZwYqkCSSOe5H4mI04uOJ4+IOCkiRkTEKFLn5x0RURe/XCNiHjBb0hbZqt2BGQWGlMfTwA6SVsv+\n/+xOnXSclym/ydYXgRsKjCUXSXuTTp/uHxGvFx1PpSLinxGxbkSMyv5m5wDbZX8LVePEUB07AV8g\n/dp+IJv2KTqoJvFN4DJJDwEfAH5ScDwVyVo5VwPTgH+S/hb77TANki4H7gO2kDRH0hHAKcCekh4n\ntYBOKTLG7nQT+1nAmsDt2d/rbwoNshvdxF77162fFpSZmfUFtxjMzKwDJwYzM+vAicHMzDpwYjAz\nsw6cGMzMrAMnBrMqkTSqq1EwzeqNE4OZmXXgxGBWA5I2zQb2G1N0LGZ5rVR0AGaNJhui4wrg8FqO\ngGlWK04MZtXVQhoz6NMRUS/jNpl14FNJZtX1CmmAvJ2LDsRsebnFYFZdS4BPAX+W9GpE/KHogMzy\ncmIwq7KIeC27AdLtWXK4seiYzPLw6KpmZtaB+xjMzKwDJwYzM+vAicHMzDpwYjAzsw6cGMzMrAMn\nBjMz68CJwczMOvh/OthswD8YlboAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7gHgW3fELn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As this is a breast cancer dataset, setting clusters to 2, either cancer present or not present.\n",
        "kmeans = KMeans(n_clusters=2,random_state=2) #adding random state for reproducability\n",
        "#Now doing pca on data without standardisation and again pca after standardisation\n",
        "pca = PCA(n_components=30) #Same as the number of features\n",
        "#Now standardising the data before implementing the pca\n",
        "scaler = StandardScaler()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0jSXUw24CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#running standardisation on data\n",
        "df_std = scaler.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--CO7_oI6o4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0e5d1c28-f70f-45b8-fb42-6334055186c7"
      },
      "source": [
        "#running pca on regular data\n",
        "pca_df = pca.fit_transform(df)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First three pca components account for maximum variance"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.82044672e-01 1.61764899e-02 1.55751075e-03 1.20931964e-04\n",
            " 8.82724536e-05 6.64883951e-06 4.01713682e-06 8.22017197e-07\n",
            " 3.44135279e-07 1.86018721e-07 6.99473205e-08 1.65908880e-08\n",
            " 6.99641650e-09 4.78318306e-09 2.93549214e-09 1.41684927e-09\n",
            " 8.29577731e-10 5.20405883e-10 4.08463983e-10 3.63313378e-10\n",
            " 1.72849737e-10 1.27487508e-10 7.72682973e-11 6.28357718e-11\n",
            " 3.57302295e-11 2.76396041e-11 8.14452259e-12 6.30211541e-12\n",
            " 4.43666945e-12 1.55344680e-12]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98204467, 0.99822116, 0.99977867, 0.9998996 , 0.99998788,\n",
              "       0.99999453, 0.99999854, 0.99999936, 0.99999971, 0.99999989,\n",
              "       0.99999996, 0.99999998, 0.99999999, 0.99999999, 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiNas9m0_zcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ebeab6ac-75b2-4432-9cb1-21bf39039ae8"
      },
      "source": [
        "#running pca on standardised data\n",
        "pca_df_std = pca.fit_transform(df_std)\n",
        "print(pca.explained_variance_ratio_)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "#First 10 pca components account for maximum variance"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.42720256e-01 1.89711820e-01 9.39316326e-02 6.60213492e-02\n",
            " 5.49576849e-02 4.02452204e-02 2.25073371e-02 1.58872380e-02\n",
            " 1.38964937e-02 1.16897819e-02 9.79718988e-03 8.70537901e-03\n",
            " 8.04524987e-03 5.23365745e-03 3.13783217e-03 2.66209337e-03\n",
            " 1.97996793e-03 1.75395945e-03 1.64925306e-03 1.03864675e-03\n",
            " 9.99096464e-04 9.14646751e-04 8.11361259e-04 6.01833567e-04\n",
            " 5.16042379e-04 2.72587995e-04 2.30015463e-04 5.29779290e-05\n",
            " 2.49601032e-05 4.43482743e-06]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44272026, 0.63243208, 0.72636371, 0.79238506, 0.84734274,\n",
              "       0.88758796, 0.9100953 , 0.92598254, 0.93987903, 0.95156881,\n",
              "       0.961366  , 0.97007138, 0.97811663, 0.98335029, 0.98648812,\n",
              "       0.98915022, 0.99113018, 0.99288414, 0.9945334 , 0.99557204,\n",
              "       0.99657114, 0.99748579, 0.99829715, 0.99889898, 0.99941502,\n",
              "       0.99968761, 0.99991763, 0.99997061, 0.99999557, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaI2O6wBsy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now running kmeans on pca transformation data as per maximum contained variance\n",
        "kmeans_nopca = kmeans.fit_predict(df) #without pca implemented on data\n",
        "pca_kmeans_nostd = kmeans.fit_predict(pca_df[:,:3]) #top 3 pca components cover the maximum variance\n",
        "pca_kmeans_std = kmeans.fit_predict(pca_df_std[:,:10]) #top 10 pca components cover the maximum variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1cDodIkEamt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the results and creating a dataframe\n",
        "kmeans_results = pd.DataFrame({'diagnosis':target,'kmeans_nopca':kmeans_nopca,'pca_kmeans_nostd':pca_kmeans_nostd,'pca_kmeans_std':pca_kmeans_std})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6S3jQAhge8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4383a8bb-e4ea-4bd5-9f0a-9075d0600610"
      },
      "source": [
        "kmeans_results.tail()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>kmeans_nopca</th>\n",
              "      <th>pca_kmeans_nostd</th>\n",
              "      <th>pca_kmeans_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>M</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    diagnosis  kmeans_nopca  pca_kmeans_nostd  pca_kmeans_std\n",
              "564         M             0                 0               0\n",
              "565         M             0                 0               0\n",
              "566         M             1                 1               0\n",
              "567         M             0                 0               0\n",
              "568         B             1                 1               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFBDOF5TiCMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "da05f8ed-6133-4891-abe0-1d235de91e84"
      },
      "source": [
        "kmeans_results.diagnosis.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXGOSzUOrbpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans_results['int_diagnosis'] = kmeans_results['diagnosis'].replace({'B':1,'M':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYEKpQYLsfeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_columns = ['kmeans_nopca', 'pca_kmeans_nostd', 'pca_kmeans_std', 'int_diagnosis']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnSWwi6JtF15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "694ce1e2-b050-441e-e380-c0f9d4ff473d"
      },
      "source": [
        "kmeans_results[result_columns].sum()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca        438\n",
              "pca_kmeans_nostd    438\n",
              "pca_kmeans_std      380\n",
              "int_diagnosis       357\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588xI_r8sOR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bce81124-ed2e-45e5-c2a1-4c1311b23165"
      },
      "source": [
        "kmeans_results[kmeans_results.diagnosis=='M'][result_columns].sum()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca        82\n",
              "pca_kmeans_nostd    82\n",
              "pca_kmeans_std      37\n",
              "int_diagnosis        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO8FOo7XtCGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2803922a-dbce-4f9f-d1ba-e1a4439d25b9"
      },
      "source": [
        "kmeans_results[kmeans_results.diagnosis=='B'][result_columns].sum()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kmeans_nopca        356\n",
              "pca_kmeans_nostd    356\n",
              "pca_kmeans_std      343\n",
              "int_diagnosis       357\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIpp3TkaJyRR",
        "colab_type": "text"
      },
      "source": [
        "**Based on the above results. The kmeans has a good true positive rate for all three types of transformation. But it has a high false positive for data without pca and data with pca but no standardisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskC80k3OKMA",
        "colab_type": "text"
      },
      "source": [
        "# You take it from here!\n",
        "\n",
        "See what you can come up with. You have all the know-how! \n",
        "\n",
        "- You might want to do some data exploration to see if you can find specific columns that will help you find distinct clusters of cells\n",
        "- You might want to do PCA on this data to see if that helps you find distinct linearly-separable clusters.\n",
        "  - (In the real world, truly linearly-separable clusters are rare.)\n",
        "- You might want to use an elbow chart to decide on the number of clusters to use.\n",
        "- You might want to use a scree plot to decide how many principal components to include in your clustering.\n",
        "- You might want to standardize your data before PCA (If you decide to use PCA). \n",
        "\n",
        "## Manage your time and don't spend it all on data exploration or something like that. You got this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1AeAK8PNah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBwVaGOOYsq",
        "colab_type": "text"
      },
      "source": [
        "# Stretch Goal:\n",
        "\n",
        "Once you are satisfied with your clustering, go back and add back in the labels from the original dataset to check how accurate your clustering was. Remember that this will not be a possibility in true unsupervised learning, but it might be a helpful for your learning to be able to check your work against the \"ground truth\". Try different approaches and see which one is the most successful and try understand why that might be the case. If you go back and try different methods don't ever include the actual \"diagnosis\" labels in your clustering or PCA.\n",
        "\n",
        "**Side Note** Data Science is never DONE. You just reach a point where the cost isn't worth the benefit anymore. There's always more moderate to small improvements that we could make. Don't be a perfectionist, be a pragmatist."
      ]
    }
  ]
}