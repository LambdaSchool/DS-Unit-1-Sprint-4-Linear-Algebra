{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering and NFL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xi9ks-kq_j8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PCA Notes\n",
        "\n",
        "## Principal Components are not a re-labeling of the original features\n",
        "\n",
        "I saw some confusion yesterday about what the new Principal Components are that come out of our PCA transformations. Principal Components are a linear combination of any and all dimensions (features) that will increase their variance, this means that PCs are made up of a mixture of features --mostly the ones with the highest variance, but also smaller parts from other features. This means that they are not comparable to the original features of our $X$ matrix. In cases where we're not reducing dimensionality that much (like the Iris dataset) our Principal Components might be extremely similar to the original features (since there's not that many to pull from) but don't think of them in that way, think of them as a completely new dataset that we can't really apply \n",
        "\n",
        "## PCA does not make predictions\n",
        "\n",
        "I would not call PCA a \"machine learning algorithm\" in that it does not try to make any predictions. We can't calculate any accuracy measure. You can call it an algorithm, you can call it a preprocessing technique or method, but it's not truly making predictions. This may have been confusing due to the fact that the Iris dataset had labels, but PCA is just re-organizing points in space, it's not making any predictions.\n",
        "\n",
        "## PCA doesn't standardize the data for you\n",
        "\n",
        "You'll notice in the \"from scratch\" implementation of PCA that I did in class yesterday that in that example I did not divide the points by the standard deviation. I believe you'll get a slightly different set of points if you choose to divide by the standard deviation (I think this might be what A Apte was seeing yesterday when he tried both methods and found that they looked different. It could be something else entirely, but that's my first guess at what could be going on.)\n",
        "\n",
        "The Sklearn implementation does not standardize the points for you as part of the process. You can either do this yourself \"by hand\" or you can use other sklearn methods like this preprocessing step which will automatically standardize your data to have a mean of 0 and a standard deviation of 1. You have to do this **before** you pass your data to PCA.\n",
        "\n",
        "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>\n",
        "\n",
        "## PCA does not retain 100% of the information of the original dataset. \n",
        "\n",
        "Each component explains a certain % of the variance of the original dataset. PCA tries to maximize that variance, but you might need to use more than 2 components. \n",
        "\n",
        "Typically you want to use enough components in your analysis to keep the explained variance > 90%.\n",
        "\n",
        "So we're trading off losing a small-medium amount of predictive power for a reduction in dimensions/size.\n",
        "\n",
        "## Intro to Scree Plots\n",
        "\n",
        "![Scree Plot](http://www.ryanleeallred.com/wp-content/uploads/2019/01/scree-plot.png)\n",
        "\n",
        "![Variance Explained](http://www.ryanleeallred.com/wp-content/uploads/2019/01/variance-explained.png)\n",
        "\n",
        "![scree plot and variance explained](http://www.ryanleeallred.com/wp-content/uploads/2019/01/scree-plot-2.png)\n",
        "\n",
        "These scree plots came from analysis that began with a lot more features than we were working with yesterday:\n",
        "\n",
        "<https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/>"
      ]
    },
    {
      "metadata": {
        "id": "TBXSEvwbODsA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Machine Learning (Overview)\n",
        "\n",
        "How do you know what kind of Machine Learning that you're doing? What algorithm should you pick? \n",
        "\n",
        "This decision is driven driven by:\n",
        "\n",
        "1) The attributes of your dataset\n",
        "\n",
        "2) What you want to predict\n",
        "\n",
        "- ## Supervised Learning: \n",
        "Supervised Learning is used when training data outputs are labelled. The output is the thing that you're trying to predict.\n",
        "  - ### Classification\n",
        "  Classification algorithms try to predict the correct category (or class) from a given set of categories.\n",
        "  - ### Regression\n",
        "  Regression algorithms predict a continuous or semi-continuous value. (Not to be confused with _Linear_ Regression)\n",
        "-  ## Unsupervised Learning\n",
        "  - ### Clustering\n",
        "  Identifying groupings of related observations. This is our topic for today!\n",
        "  - ### Dimensionality Reduction\n",
        "  Takes a high-dimensionality dataset and reduces the number of variables taken into consideration via methods of feature selection and feature extraction.\n",
        "  - ### Association Rule Learning\n",
        "  Association is a method of discovering relationships between observations in a dataset. (between ovservations or features, not just relationships between explanatory variables and a single output variable. )\n",
        "- ## Reinforcement Learning\n",
        "  A form of machine learning where an \"agent\" interacts with its environment and is rewarded for correct behavior and penalized for incorrect behavior. Over many iterations the agent learns the behavior that results in the greatest reward and smallest punishment. "
      ]
    },
    {
      "metadata": {
        "id": "2nPOjACaubCv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Memorize This!\n",
        "\n",
        "**Supervised**: Labelled outputs\n",
        "- **Classification**: Discrete output cagetories\n",
        "- **Regression**: Continuous output values\n",
        "\n",
        "**Unsupervised**: Outputs are not labelled\n",
        "\n",
        "**Reinforcement**: Rewards/punishments for \"behaviors\""
      ]
    },
    {
      "metadata": {
        "id": "K9YeIVBQoAJR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Examples\n",
        "\n",
        "/polly \"I know what Kaggle is.\" \"Yes\" \"No\"\n",
        "\n",
        "/polly \"I have made a submission to a Kaggle competition before.\" \"Yes\" \"No\"\n",
        "\n",
        "Show some kaggle datasets and show how the training data has labels but the testing data does not.\n",
        "\n",
        "## [Classification Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        " - Think Titanic Dataset\n",
        "\n",
        "## [Regression Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        "- Think Home Price Prediction\n",
        "\n",
        "## [Unsupervised Learning Examples](http://www.lsi.upc.edu/~bejar/apren/docum/trans/09-clusterej-eng.pdf)\n",
        "\n",
        "- Think Iris Dataset (clustering)"
      ]
    },
    {
      "metadata": {
        "id": "yzdWvN9iugsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ML Cheat Sheets\n",
        "\n",
        "![Microsoft Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet-small_v_0_6-01.png)\n",
        "\n",
        "![PerceptionBox Cheat Sheet](https://perceptionbox.io/blog/content/images/2018/10/Machine-learning-algorithms-cheat-sheet.png)\n",
        "\n",
        "This one does not group them by supervised, unsupervised, regression, classification, etc. But it gives you an idea of the different families of algorithms.\n",
        "\n",
        "![Algorithm Map](https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png?w=816&h=521&zoom=2)\n"
      ]
    },
    {
      "metadata": {
        "id": "ZisUEU-B-tTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CY95oSIT-5ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clustering \n",
        "\n",
        "Clustering falls into the category of unsupervised learning. This is because there is nothing in our training data that designates the correct cluster that a data point should belong to beforehand. In fact, there's not even a \"correct\" _**number**_ of clusters to assign our points to. We will discuss some heuristics for choosing an **appropriate** number of clusters, but this (as in much of data science) is an area where there is no cut and dry right and wrong answer. \n",
        "\n",
        "Remember: \"All models are wrong, but some models are useful.\" Data science is all about acknowledging where your model might be wrong while still pursuing something useful. \n",
        "\n",
        "## Why Clustering?\n",
        "\n",
        "Clustering answers questions about how similar or dissimilar our \"data objects\" are. Clustering is one of the most effective methods for summarizing datasets with this question in mind. Clustering can be thought of as a sort of \"unsupervised classification.\" You will likely never deploy a clustering model to a production environment, they're too unreliable. Clustering is more useful as a tool for data exploration than a model for making predictions. \n",
        "\n",
        "## “Clustering isn’t hard—it’s either easy, or not interesting”\n",
        "\n",
        "If a good clustering exists, then it usually can be efficiently found. Clustering is the most difficult when clear clusters don't exist in the first place. In that case you should question whether or not clustering is the most appropriate or useful method. \n",
        "\n",
        "The purpose of clustering is to group data points that are similar along certain specified dimensions (attributes). \"Similarity\" is defined as the points being close together in some n-dimensional space. \n",
        "\n",
        "The greater the number of dimensions, the more difficult clustering becomes because the increase in dimensions makes all points this is because measures of distance are used to determine similarity between datapoints, and the greater the dimensionality the more all points become roughly equidistant with one another. (We don't have time to go further into this or demonstrate this, but clustering suffers from performance and interpretability issues in a high number of dimensions). Some of these challenges can be rectified by choosing an appropriate measure of \"distance\" between data points. For example, using clustering for document analysis is still fairly effective even though the analysis is of a highly-dimenaional space. \n",
        "\n",
        "# Types of Clustering:\n",
        "\n",
        "## Hierarchical:\n",
        "\n",
        "  - Agglomerative: start with individual points and combine them into larger and larger clusters\n",
        "  \n",
        "  - Divisive: Start with one cluster and divide the points into smaller clusters.\n",
        "\n",
        "## Point Assignment:\n",
        "\n",
        "  - We decide on a number of clusters out of the gate, and assign points to that number of clusters.\n",
        "\n",
        "# Hard vs Soft Clustering\n",
        "\n",
        "Hard Clustering assigns a point to a cluster\n",
        "\n",
        "Soft Clustering assigns each point a probability that it's in a given cluster.\n",
        "\n",
        "We're going to only deal with hard clustering, it's the more traditional approach. \n",
        "\n",
        "## Applications:\n",
        "\n",
        "Astronomy: There's too much data from space for us to look at each individual start and galaxy and categorize it, but we can cluster them intro groups based on their observable attributes. \n",
        "\n",
        "[SkyCat](http://www.eso.org/sci/observing/tools/skycat.html)\n",
        "\n",
        "[Sloan Digital Sky Survey](https://www.sdss.org/)\n",
        "\n",
        "Document Classification / Grouping - We'll need to study a little bit of NLP before we can get into this. \n",
        "\n",
        "## There are a lot of clustering algorithms. \n",
        "\n",
        "YOU DON'T NEED TO BE ABLE TO CODE ALL OF THEM FROM SCRATCH IN ORDER TO APPLY THEM OR EVEN TO UNDERSTAND THEM. FOCUS ON LEARNING THINGS WITHIN THE CONTEXT OF A PROBLEM YOU ARE TRYING TO SOLVE AND ONLY LEARN THOSE THINGS THAT WILL HELP YOU SOLVE THE PROBLEM. \n",
        "\n",
        "## Distance Measures\n",
        "\n",
        "Did you know that there are distance measures other than euclidean distance?\n",
        "\n",
        "- Euclidean\n",
        "- Cosine\n",
        "- Jaccard\n",
        "- Edit Distance\n",
        "- Etc. \n",
        "\n",
        "Clustering traditionally uses Euclidean Distance, but this particular measure of distance breaks down in high dimensionality spaces. It's what we'll use for today. If you **LOVE**  clustering and want to put a strong focus on this area of Machine learning (at the expense of focusing strongly on others) then I would suggest further personal research into different clustering algorithms and distance measures. \n",
        "\n",
        "I want to reiterate that you don't have to use PCA and clustering in conjunction with each other. I think it's more common that they are not used together, but it can be useful in certain cases. We might try it today for fun and so reiterate how PCA is the preprocessing step, and K-means will be the main \"Machine Learning Algorithm.\"\n"
      ]
    },
    {
      "metadata": {
        "id": "OGSeEtls_QXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "![K-means Clustering](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/440px-K-means_convergence.gif)\n",
        "\n",
        "## The Process:\n",
        "\n",
        "Given a set of points in n-dimensional space we want to :\n",
        "\n",
        "1) select k random points to act as initial centroids (one point for each cluster)\n",
        "\n",
        "2) Find the cluster of points surrounding that centroid (assign points to the centroid that they lie closest to)\n",
        "\n",
        "3) Calculate a new centroid for the cluster\n",
        "\n",
        "Repeat steps 2 & 3 until the model converges. (Clusters don't change)"
      ]
    },
    {
      "metadata": {
        "id": "JnwzM7vnoIDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lets make some blobs"
      ]
    },
    {
      "metadata": {
        "id": "Lt03ADtDByNX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3fV8Cw_Eq6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Separability\n",
        "The 2D blobs below are what is called \"linearly separable\" Meaning that we could use straight lines to separate them with no errors. This is the most trivial case of of k-means clustering, but it will help us to demonstrate."
      ]
    },
    {
      "metadata": {
        "id": "Ro4aO_mPoP1v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmVwnSM1R6tz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Re-review steps of the algorithm\n",
        "\n",
        "Given a set of points in n-dimensional space we want to:\n",
        "\n",
        "1) select k random points to act as initial centroids (one point for each cluster)\n",
        "\n",
        "2) Find the cluster of points surrounding that centroid (assign points to the centroid that they lie closest to)\n",
        "\n",
        "3) Calculate a new centroid for the cluster\n",
        "\n",
        "Repeat steps 2 & 3 until the model converges. (Clusters don't change)"
      ]
    },
    {
      "metadata": {
        "id": "WzETDJC2FNDK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculating the Centroid\n",
        "\n",
        "K-means clustering is what's known as a centroid-based clustering algorithm. A centroid is an imaginary point located at the average location of all of the points in a given cluster. For example, if I wanted to find the centroid of all of the points in the above graph I would just calculate the average of the dataset's x-coordinates to find the x value of the centroid, and the average of the dataset's y-coordinates to find the y value of the centroid.\n",
        "\n",
        "If we plot the centroid on the graph you'll see that it lies in the middle of the points. You could imagine the centroid as if it is the center of gravity, or center of mass for a given cluster. Since in this example we're treating all of the points in the dataset as if they're in the same cluster, it will end up somewhere in the middle. We're just doing this to demonstrate what a centroid is. The K-means algorithm doesn't ever calculate the centroid for the entire dataset."
      ]
    },
    {
      "metadata": {
        "id": "Kv_10d39F6bK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-nJ_i-NQZIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How many centroids = K-means \n",
        "\n",
        "Since the centroid is the mean of a cluster the number of centroids to choose is the most important decision to make in \"k-means\" clustering. The K value is the number of centroids.\n",
        "\n",
        "\n",
        "### The Eyeball Method\n",
        "/polly \"How many centroids (means) should we use for this exercise?\"\n",
        "\n",
        "Congratulations, you've just been introduced to the first method of _**picking k**_ - Just graph your points and pick a number that makes sense. This gets a lot harder once you get a dimensionality higher than 3, but... Didn't we learn about some way to take high dimensional data and turn it into 2 or 3 dimensions...? 😀\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p9LlmLSySTb7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3-means clustering\n",
        "\n",
        "Lets pick k=3 and start demonstrating how this algorithm actually works algorithm rolling. \n",
        "\n",
        "The k-means algorithm works by picking 3 of the actual datapoints at random (in the simplest case) and treating those as the starting centroids. Using those centroids, 3 clusters are calculated.\n",
        "\n",
        "We then use the new clusters and calculate a new centroid for each of them. Then, using those centroids we re-cluster. We perform this process over and over again until our clusters stabilize and the centroids stop moving. Lets demonstrate."
      ]
    },
    {
      "metadata": {
        "id": "elooRWBkS4_c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAkWFsyaSNwU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDoPfxaF_OXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z56wfhQUJcAY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uIlfNFrKvuY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhT7VhaANR2C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jFqBdjYcODAx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcP4Bn-tOW33",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "weuyX9wZOmCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGueCso5SFXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lets use a library to do it: Scikit-Learn"
      ]
    },
    {
      "metadata": {
        "id": "VmyHklDKSI_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Gj9sXFDWtLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on8qYidhXaA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtVhKzHDXjqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WF5d6AP_VT4c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Important Considerations:\n",
        "\n",
        "## Choosing the appropriate clustering method \n",
        "\n",
        "We've only taught you one so stick with that for today. \n",
        "\n",
        "## Choosing appropriate dimensions to cluster along. \n",
        "\n",
        "Hmmm, what would be the best dimension to cluster along? Maybe one that helps separate the clusters the best. You can do a lot of scatterplots to examine this or you could, I dunno, use a technique that maximizes the variance along certain dimensions transforming the data into principal components and then cluster along the dimensions of the principal components. \n",
        "\n",
        "## Choosing a distance measure\n",
        "\n",
        "Euclidean is the most traditional, you'll learn the others if the occasion presents itself (it most likely won't) - If I'm being completely honest.\n",
        "\n",
        "## Choosing an appropriate k (# of clusters)\n",
        "\n",
        "THIS IS THE MOST IMPORTANT CONSIDERATION WHEN IT COMES TO K-MEANS (I mean it's in the name)\n",
        "\n",
        "[Elbow Method](http://www.ryanleeallred.com/wp-content/uploads/2019/01/elbow-method.png)\n",
        "\n",
        "On the x-axis we have number of centroids (k)\n",
        "\n",
        "On the y-axis we have \"distortion\" which is measured as the sum of squared distances of "
      ]
    },
    {
      "metadata": {
        "id": "DhMkzbIFX96q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Further Considerations\n",
        "\n",
        "## Choosing an appropriate K\n",
        "\n",
        "## Unlucky Initial Centroids\n",
        "\n",
        "Unlucky Initial Centroids can \n",
        "\n",
        "- result in a poor clustering\n",
        "- lead to a clustering that doesn't converge\n",
        "\n",
        "## Computational Complexity\n",
        "\n",
        "## What is K-means good for?\n",
        "\n",
        "- ### Mostly Round, linearly-separable blobs."
      ]
    },
    {
      "metadata": {
        "id": "QEEQapU2uRIx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# No Free Lunch\n",
        "\n",
        "The no free lunch principle states that the more an algorithm is optimized to solve one specific kind of problem, the worse it gets at solving all other kinds of problems. \n",
        "\n",
        "This means that if you want an algorithm that's really good at solving a certain problem (cluster shape for example), it usually lose some of its ability to generalize to other problems. \n",
        "\n",
        "### What does this mean for us as data scientists?\n",
        "\n",
        "1) There are always tradeoffs when selecting from different approaches. Because of this, understanding those tradeoffs and justifying your choice of methodology is just as important as actually doing the work itself.\n",
        "\n",
        "2) The only way that we can choose one approach over another is to make assumptions about our data. If we don't know anything about the characteristics of our data, then we can't make an informed choice of algorithm. \n",
        "\n",
        "Think about how we knew to use Unsupervised vs Supervised learning for the clustering problem, the choice was informed by our data. Does it have labels or not? \n",
        "\n",
        "![No Free Lunch](https://cdn-images-1.medium.com/max/1600/1*oNt9G9UpVhtyFLDBwEMf8Q.png)\n",
        "\n",
        "Density Based Clustering\n",
        "\n",
        "[DB Scan Animation](https://www.youtube.com/watch?v=h53WMIImUuc)\n",
        "\n",
        "Hard Clustering: In hard clustering, each data point either belongs to a cluster completely or not. For example, in the above example each customer is put into one group out of the 10 groups.\n",
        "\n",
        "Soft Clustering: In soft clustering, instead of putting each data point into a separate cluster, a probability or likelihood of that data point to be in those clusters is assigned. For example, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store.\n",
        "\n",
        "## Don't Get Overwhelmed! \n",
        "\n",
        "Some people spend their entire careers researching new clustering methods and improvements.\n",
        "\n",
        "## Don't be a perfectionist! \n",
        "\n",
        "There are too many techniques to master, you can't learn all of them in 7 months.\n",
        "\n",
        "## Focus on learning in the context of a problem you want to solve or a project that you are passionate about building\n"
      ]
    }
  ]
}