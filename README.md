### Vectors and Matrices
Learning the fundamentals of linear algebra is critical in order to be a well-rounded data scientist. Many algorithm implementations are highly LA-based, so a familiarity with these topics is necessary in order to comprehend those approaches.

In this lesson we will be covering some of the most basic (yet the most important) topics in linear algebra including:

* Scalars
* Vectors
* Norm
* Dot Product
* Cross Product
* Matrices
* Matrix Equality
* Matrix Multiplication
* Transpose
* Square Matrix
* Special kinds of Square Matrices
* Determinant
* Inverse
* Intro to NumPy for Linear Algebra

##### Objectives
* Student should be able to Explain why we care about linear algebra in the scope of data science
* Student should be able to Conceptualize and utilize vectors and matrices through matrix operations and properties such as: square matrix, identity matrix, transpose and inverse

### Intermediate Linear Algebra
##### Objectives
* Student should be able to Show when two vectors/matrices are orthogonal and explain the intuitive implications of orthogonality
* Student should be able to Understand the roles of covariance matrices and eigenvectors in finding a datasetâ€™s principle components

### Dimensionality Reduction Techniques
##### Objectives
* Student should be able to Recognize when p > n, and why this leads to failure of certain ML models
* Student should be able to Recognize high dimensionality data, and can employ PCA/SVD to improve model performance
* Student should be able to Understand the limitations that come with projecting data onto an eigenvector subspace

### Clustering
##### Objectives
* Student should be able to Recognize when unsupervised learning is necessary, and select and apply appropriate clustering techniques
* Student should be able to Use K-Means clustering and other centroid-based clustering algorithms
