{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Luc Guittard - Clustering assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "IHDDqaU-ove4",
        "UOLDlkLQw1pq",
        "x_VA1oxfwswY",
        "UUzSUplKwCup"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucguittard/DS-Unit-1-Sprint-4-Linear-Algebra/blob/master/Luc_Guittard_Clustering_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3rVFtGLMJM",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VS3FFSFLR3a",
        "colab_type": "text"
      },
      "source": [
        "Your assignment is to use the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" from Kaggle to try and cluster types of cancer cells. \n",
        "\n",
        "It may be helpful to use PCA to reduce the dimensions of your data first in order to obtain --but then again, maybe not. I dunno, you're the data scientist, you tell me.ðŸ¤ª \n",
        "\n",
        "Here's the original dataset for your reference:\n",
        "\n",
        "<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899RK3bBn4OE",
        "colab_type": "text"
      },
      "source": [
        "## This is a supervised learning dataset\n",
        "\n",
        "(Because it has **labels** - The \"diagnosis\" column.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5R9X6hLJQ2",
        "colab_type": "code",
        "outputId": "24e500e4-aef5-47a5-8ab3-9e6633ee3a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # You don't necessarily have to use this\n",
        "from sklearn.cluster import KMeans # You don't necessarily have to use this\n",
        "from sklearn.preprocessing import StandardScaler # You don't necessarily have to use this\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/Cancer_Cells.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDDqaU-ove4",
        "colab_type": "text"
      },
      "source": [
        "## Now it's an unsupervised learning dataset\n",
        "\n",
        "(Because we've removed the diagnosis label) - Use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86MHoPJon_aC",
        "colab_type": "code",
        "outputId": "4f94bc69-ea81-4f5d-9fc1-8270bb3a3c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df = df.drop('diagnosis', axis=1)\n",
        "df.head()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302        17.99  ...                  0.11890          NaN\n",
              "1    842517        20.57  ...                  0.08902          NaN\n",
              "2  84300903        19.69  ...                  0.08758          NaN\n",
              "3  84348301        11.42  ...                  0.17300          NaN\n",
              "4  84358402        20.29  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskC80k3OKMA",
        "colab_type": "text"
      },
      "source": [
        "# You take it from here!\n",
        "\n",
        "See what you can come up with. You have all the know-how! \n",
        "\n",
        "- You might want to do some data exploration to see if you can find specific columns that will help you find distinct clusters of cells\n",
        "- You might want to do PCA on this data to see if that helps you find distinct linearly-separable clusters.\n",
        "  - (In the real world, truly linearly-separable clusters are rare.)\n",
        "- You might want to use an elbow chart to decide on the number of clusters to use.\n",
        "- You might want to use a scree plot to decide how many principal components to include in your clustering.\n",
        "- You might want to standardize your data before PCA (If you decide to use PCA). \n",
        "\n",
        "## Manage your time and don't spend it all on data exploration or something like that. You got this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBD1E5bR0E6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "a5be2779-f140-4e54-fb7c-43220429f2ac"
      },
      "source": [
        "# cleaning\n",
        "\n",
        "df.dtypes\n",
        "df.isna().sum()\n",
        "df = df.drop('Unnamed: 32', axis = 1)\n",
        "df.isna().sum()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                         0\n",
              "radius_mean                0\n",
              "texture_mean               0\n",
              "perimeter_mean             0\n",
              "area_mean                  0\n",
              "smoothness_mean            0\n",
              "compactness_mean           0\n",
              "concavity_mean             0\n",
              "concave points_mean        0\n",
              "symmetry_mean              0\n",
              "fractal_dimension_mean     0\n",
              "radius_se                  0\n",
              "texture_se                 0\n",
              "perimeter_se               0\n",
              "area_se                    0\n",
              "smoothness_se              0\n",
              "compactness_se             0\n",
              "concavity_se               0\n",
              "concave points_se          0\n",
              "symmetry_se                0\n",
              "fractal_dimension_se       0\n",
              "radius_worst               0\n",
              "texture_worst              0\n",
              "perimeter_worst            0\n",
              "area_worst                 0\n",
              "smoothness_worst           0\n",
              "compactness_worst          0\n",
              "concavity_worst            0\n",
              "concave points_worst       0\n",
              "symmetry_worst             0\n",
              "fractal_dimension_worst    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOLDlkLQw1pq",
        "colab_type": "text"
      },
      "source": [
        "## PCA / Cluster Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1AeAK8PNah",
        "colab_type": "code",
        "outputId": "7e258f57-6a08-4a67-c414-8f7a9698a80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Principal Component Analysis\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# define a matrix\n",
        "data = df.values\n",
        "print(\"Data: \\n\", data)\n",
        "\n",
        "# Standardize the Data\n",
        "# Instantiate a Standard Scaler object\n",
        "scaler = StandardScaler()\n",
        "# Use the object to fit_transform our data\n",
        "Z = scaler.fit_transform(data)\n",
        "print(\"\\n Standardized Data: \\n\", data)\n",
        "\n",
        "# create the PCA instance\n",
        "pca = PCA(.90) #Hey PCA, pick the minimum no. of principle components that account for 90% of variance -> returns 8 PCs\n",
        "\n",
        "# fit on data\n",
        "pca.fit(Z)\n",
        "\n",
        "#Count no. of principle components\n",
        "print(\"\\n No. of PCs: \\n\",pca.n_components_)\n",
        "\n",
        "# access values and vectors\n",
        "print(\"\\n Eigenvectors: \\n\", pca.components_)\n",
        "print(\"\\n Eigenvalues: \\n\",pca.explained_variance_)\n",
        "\n",
        "# transform data\n",
        "B = pca.transform(Z)\n",
        "print(B.shape,\"\\n Projected Data: \\n\", B)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data: \n",
            " [[8.4230200e+05 1.7990000e+01 1.0380000e+01 ... 2.6540000e-01\n",
            "  4.6010000e-01 1.1890000e-01]\n",
            " [8.4251700e+05 2.0570000e+01 1.7770000e+01 ... 1.8600000e-01\n",
            "  2.7500000e-01 8.9020000e-02]\n",
            " [8.4300903e+07 1.9690000e+01 2.1250000e+01 ... 2.4300000e-01\n",
            "  3.6130000e-01 8.7580000e-02]\n",
            " ...\n",
            " [9.2695400e+05 1.6600000e+01 2.8080000e+01 ... 1.4180000e-01\n",
            "  2.2180000e-01 7.8200000e-02]\n",
            " [9.2724100e+05 2.0600000e+01 2.9330000e+01 ... 2.6500000e-01\n",
            "  4.0870000e-01 1.2400000e-01]\n",
            " [9.2751000e+04 7.7600000e+00 2.4540000e+01 ... 0.0000000e+00\n",
            "  2.8710000e-01 7.0390000e-02]]\n",
            "\n",
            " Standardized Data: \n",
            " [[8.4230200e+05 1.7990000e+01 1.0380000e+01 ... 2.6540000e-01\n",
            "  4.6010000e-01 1.1890000e-01]\n",
            " [8.4251700e+05 2.0570000e+01 1.7770000e+01 ... 1.8600000e-01\n",
            "  2.7500000e-01 8.9020000e-02]\n",
            " [8.4300903e+07 1.9690000e+01 2.1250000e+01 ... 2.4300000e-01\n",
            "  3.6130000e-01 8.7580000e-02]\n",
            " ...\n",
            " [9.2695400e+05 1.6600000e+01 2.8080000e+01 ... 1.4180000e-01\n",
            "  2.2180000e-01 7.8200000e-02]\n",
            " [9.2724100e+05 2.0600000e+01 2.9330000e+01 ... 2.6500000e-01\n",
            "  4.0870000e-01 1.2400000e-01]\n",
            " [9.2751000e+04 7.7600000e+00 2.4540000e+01 ... 0.0000000e+00\n",
            "  2.8710000e-01 7.0390000e-02]]\n",
            "\n",
            " No. of PCs: \n",
            " 8\n",
            "\n",
            " Eigenvectors: \n",
            " [[ 2.29121629e-02  2.18913018e-01  1.03843884e-01  2.27534912e-01\n",
            "   2.21045775e-01  1.42414707e-01  2.39067299e-01  2.58280254e-01\n",
            "   2.60738114e-01  1.37977741e-01  6.41477914e-02  2.06117467e-01\n",
            "   1.74133924e-02  2.11446519e-01  2.03076420e-01  1.46782103e-02\n",
            "   1.70288397e-01  1.53543673e-01  1.83406751e-01  4.24155186e-02\n",
            "   1.02496073e-01  2.28009352e-01  1.04515447e-01  2.36637339e-01\n",
            "   2.24932135e-01  1.27824406e-01  2.09884561e-01  2.28602175e-01\n",
            "   2.50746203e-01  1.22679932e-01  1.31560243e-01]\n",
            " [-3.40684907e-02 -2.33271401e-01 -6.00441986e-02 -2.14589002e-01\n",
            "  -2.30668816e-01  1.86422211e-01  1.52454726e-01  6.05416253e-02\n",
            "  -3.41673916e-02  1.90684979e-01  3.66531055e-01 -1.05935702e-01\n",
            "   8.95477887e-02 -8.98070428e-02 -1.52771289e-01  2.03189876e-01\n",
            "   2.32503362e-01  1.96846081e-01  1.29965181e-01  1.83558627e-01\n",
            "   2.79584139e-01 -2.19296044e-01 -4.55012225e-02 -1.99295985e-01\n",
            "  -2.18985461e-01  1.72562959e-01  1.44253637e-01  9.85265243e-02\n",
            "  -7.53436736e-03  1.42619436e-01  2.75702077e-01]\n",
            " [ 9.69384357e-02 -1.13937863e-02  6.68923422e-02 -1.21247907e-02\n",
            "   2.62931497e-02 -1.03182400e-01 -7.47686230e-02  1.75873594e-03\n",
            "  -2.75796069e-02 -4.09620321e-02 -2.08178746e-02  2.66917221e-01\n",
            "   3.71439885e-01  2.64925682e-01  2.15790250e-01  3.11787845e-01\n",
            "   1.54557465e-01  1.76560052e-01  2.23850479e-01  2.85265066e-01\n",
            "   2.11893354e-01 -4.94063395e-02 -3.98289339e-02 -5.04319449e-02\n",
            "  -1.31888912e-02 -2.55328751e-01 -2.34513609e-01 -1.72024941e-01\n",
            "  -1.70480673e-01 -2.70515902e-01 -2.29474476e-01]\n",
            " [-2.65980453e-02  4.21879503e-02 -6.02954308e-01  4.27527972e-02\n",
            "   5.41147240e-02  1.58098177e-01  3.18181168e-02  1.94971242e-02\n",
            "   6.57853527e-02  6.75025426e-02  4.79578564e-02  9.91144458e-02\n",
            "  -3.56497230e-01  9.02930555e-02  1.08568705e-01  4.43686639e-02\n",
            "  -2.64253603e-02  2.24829120e-03  7.52522324e-02  4.69361262e-02\n",
            "   1.62124502e-02  1.56597047e-02 -6.33119655e-01  1.40685717e-02\n",
            "   2.59706715e-02  1.45233595e-02 -9.25621677e-02 -7.48071883e-02\n",
            "   5.30597982e-03 -3.71294655e-02 -7.89714892e-02]\n",
            " [-1.13275871e-02  3.81298606e-02 -4.90914502e-02  3.77155917e-02\n",
            "   1.05622290e-02 -3.65750055e-01  1.17866372e-02  8.65125060e-02\n",
            "  -4.36674117e-02 -3.05378893e-01 -4.47679064e-02 -1.54254367e-01\n",
            "  -1.90001500e-01 -1.20703357e-01 -1.27765023e-01 -2.32745603e-01\n",
            "   2.80298048e-01  3.54164595e-01  1.95758558e-01 -2.51331178e-01\n",
            "   2.63395188e-01 -4.28003421e-03 -9.25518597e-02  7.59914369e-03\n",
            "  -2.74135953e-02 -3.25860028e-01  1.21503371e-01  1.88280510e-01\n",
            "   4.31235727e-02 -2.44245936e-01  9.36990782e-02]\n",
            " [ 3.16733438e-01 -2.95885214e-02  3.13943235e-02 -2.83940081e-02\n",
            "  -6.11315511e-03  2.62508993e-01  4.90389406e-03  2.35633784e-03\n",
            "   3.45092730e-02 -3.35082168e-01  1.12784169e-01  2.32611990e-02\n",
            "  -2.28569116e-02 -3.82015070e-03  5.19588353e-02  3.30867850e-01\n",
            "  -6.67881201e-02 -4.96991037e-02  2.31975263e-02 -4.77530515e-01\n",
            "   4.84623730e-02 -4.52173650e-03  4.51745156e-02 -1.29211658e-02\n",
            "   2.40333377e-02  3.65048687e-01 -3.40427143e-02 -1.79620401e-02\n",
            "   2.95490998e-02 -4.51404312e-01  9.24796979e-02]\n",
            " [ 9.07115632e-01 -4.22987777e-02  1.49935618e-02 -4.35888242e-02\n",
            "  -2.89256668e-02 -1.40340362e-01 -4.53031106e-02 -3.25530646e-02\n",
            "  -8.14216298e-02  1.18259236e-01 -4.10588768e-02  1.67882718e-02\n",
            "  -1.90267647e-01  1.95081762e-02  5.65606078e-02 -6.78348099e-02\n",
            "   2.22220211e-02  3.36810725e-02 -3.78517870e-02  1.18403261e-01\n",
            "  -1.57602244e-02 -1.66458140e-02 -9.46012397e-03 -1.45260986e-02\n",
            "  -7.37260183e-04 -6.70682168e-02  5.07556727e-02  3.52007117e-02\n",
            "  -2.07238959e-02  2.34014329e-01  3.47167538e-02]\n",
            " [-9.63624150e-02 -1.16427419e-01  1.87548206e-03 -1.06272097e-01\n",
            "  -4.74145683e-02 -1.23541189e-01  4.31459676e-02 -1.02436021e-01\n",
            "  -1.36923237e-01 -9.88745312e-02  3.06499872e-01  3.07415709e-01\n",
            "  -5.26324770e-02  3.11265679e-01  3.34287959e-01 -2.60833914e-01\n",
            "   2.10019435e-02 -2.19193299e-01 -3.70217167e-01 -8.48547675e-02\n",
            "   1.94418818e-01 -7.50830655e-03  6.61764040e-03  2.16248810e-03\n",
            "   6.61731861e-02 -1.16496117e-01  1.36509363e-01 -6.70857444e-02\n",
            "  -1.66500918e-01 -4.14396326e-02  3.72034479e-01]]\n",
            "\n",
            " Eigenvalues: \n",
            " [13.31145188  5.70683496  2.84038694  1.98484548  1.65171815  1.23684643\n",
            "  0.97999555  0.67293563]\n",
            "(569, 8) \n",
            " Projected Data: \n",
            " [[ 9.18319983  1.97127137 -1.17162471 ... -1.37228049  0.37184363\n",
            "   2.18052922]\n",
            " [ 2.38329766 -3.75345877 -0.58022866 ... -0.12661612 -0.28826986\n",
            "   0.04493484]\n",
            " [ 5.74247239 -1.08035048 -0.53308788 ... -0.401855    0.4631876\n",
            "  -0.7152086 ]\n",
            " ...\n",
            " [ 1.2518901  -1.89397674  0.53446685 ...  0.39471327 -0.43780583\n",
            "  -0.17866837]\n",
            " [10.36503528  1.69639755 -1.90741785 ... -0.65671962 -0.19526395\n",
            "   0.27520221]\n",
            " [-5.47826365 -0.67278804  1.47716504 ... -1.57207609  0.60034041\n",
            "   1.63559151]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGjnGk6f6nQ_",
        "colab_type": "code",
        "outputId": "dacee7b6-9876-42ff-cbf6-b3cfd3523c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0    842302        17.99  ...          0.4601                  0.11890\n",
              "1    842517        20.57  ...          0.2750                  0.08902\n",
              "2  84300903        19.69  ...          0.3613                  0.08758\n",
              "3  84348301        11.42  ...          0.6638                  0.17300\n",
              "4  84358402        20.29  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlAuvksZ4AsF",
        "colab_type": "code",
        "outputId": "8c909838-c046-47bf-f1b6-33c7c8e7323c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#plotting the projection \n",
        "x = B[:,0:1] \n",
        "y = B[:,1:2]\n",
        "#print()\n",
        "df2 = pd.DataFrame(x)\n",
        "df3 = pd.DataFrame(y)\n",
        "\n",
        "plt.scatter(df2, df3)\n",
        "plt.title(\"Data After PCA\")\n",
        "plt.xlabel('PC1')\n",
        "plt.show()\n",
        "#difficulty interpreting this; 1 potential linearly-seperable cluster "
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEWCAYAAAByqrw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N09foXd7kV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pca = PCA().fit(Z) #Z is standardized data matrix\n",
        "# figure = plt.subplots(figsize = (10,10))\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# plt.xlabel('number of components')\n",
        "# plt.ylabel('cumulative explained variance')\n",
        "# plt.grid(color='k', which='major', axis='both');\n",
        "\n",
        "#appears to concur with quick method above - 8 PCs seems like a good number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ITHoS1mkTCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "19ce1bd2-d953-40c4-b10f-b06f2e6bdbba"
      },
      "source": [
        "#Elbow Method of determining number of clusters/centroids\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(df)\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "    \n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show() \n",
        "\n",
        "# setting between three & eight categories if working with pc_df; \n",
        "#  three cats if working with df (sans label) "
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_VA1oxfwswY",
        "colab_type": "text"
      },
      "source": [
        "## Clustering , lost - disregard this sxn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb5GirKIEWlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clustering: KMeans approach\n",
        "from sklearn.cluster import KMeans \n",
        "kmeans = KMeans(n_clusters=1)  # one cluster suffices from analysis of plotted PCA projection\n",
        "kmeans.fit(df)\n",
        "labels = kmeans.labels_\n",
        "new_series = pd.Series(labels)\n",
        "df4 = df\n",
        "df4['clusters'] = new_series.values\n",
        "df4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAU6lu8oHu4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Calculate the Nearest Centroid to each data point\n",
        "def find_nearest_centroid(pc_df, centroids, iteration):\n",
        " \n",
        "  # calculate the distances between each point and each centroid\n",
        "  distances = distance.cdist(pc_df, centroids, 'euclidean')\n",
        "  \n",
        "  # Get nearest centroid to each point based on distance\n",
        "  nearest_centroids = np.argmin(distances, axis=1)\n",
        "\n",
        "  se = pd.Series(nearest_centroids)\n",
        "  pc_df['cluster_'+iteration] = se.values\n",
        "  \n",
        "  return pc_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad39deKPMz1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# points = df[['radius_mean', 'texture_mean','perimeter_mean', 'area_mean',\n",
        "#             'smoothness_mean', 'compactness_mean', 'concavity_mean', 'symmetry_mean']]\n",
        "# points.head() #picking 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX9ERijdX66i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pc_data = B  #principle component data optained from PCA\n",
        "pc_df = pd.DataFrame(B, columns=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"])\n",
        "pc_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgeIkl5THsOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroids = pc_df.sample(1)\n",
        "print(centroids.head())\n",
        "\n",
        "first_pass = find_nearest_centroid(pc_df.select_dtypes(exclude='int64'), centroids, '1')\n",
        "first_pass.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUMPOnFXiPEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pc_df.a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdY4breGhQEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "def find_nearest_centroid(df, centroids):\n",
        "  last_centroids = [np.random.choice([0,1,2])] * df.shape[0]\n",
        "  df_a = df.copy()\n",
        "  i = 0\n",
        "  \n",
        "  while True:\n",
        "    if i>0:\n",
        "      centroids = get_centroids(df_a, 'cluster' + str(i-1))\n",
        "\n",
        "    distances = spatial.distance.cdist(df_a[['a', 'b']], centroids[['a', 'b']])\n",
        "    nearest_centroids = np.argmin(distances, axis=1)\n",
        "    \n",
        "    df_a['cluster' + str(i)] = nearest_centroids\n",
        "    \n",
        "    if (list(nearest_centroids) == list(last_centroids)):\n",
        "      return df_a\n",
        "    else:\n",
        "      i +=1\n",
        "      last_centroids = nearest_centroids\n",
        "      \n",
        "def get_centroids(df, column_header):\n",
        "  x = [df.a[df[column_header]==0].mean(), df.a[df[column_header]==1].mean(), df.a[df[column_header]==2].mean()]\n",
        "  y = [df.b[df[column_header]==0].mean(), df.b[df[column_header]==1].mean(), df.b[df[column_header]==2].mean()]\n",
        "  data = {'a': a, 'b' : b}\n",
        "  return pd.DataFrame(data)\n",
        "  \n",
        "  \n",
        "find_nearest_centroid(pc_df, centroids).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUgp2hGlgDKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_clusters(df, column_header, centroids):\n",
        "  colors = {0:'red', 1:'green', 2:'yellow'}\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(centroids.iloc[0].a, centroids.iloc[0].b, \"ok\")\n",
        "  grouped = df.groupby(column_header)\n",
        "  for key, group in grouped:\n",
        "      group.plot(ax=ax, kind='scatter', a='a', b='b', label=key, color=colors[key])\n",
        "  plt.show()\n",
        "  \n",
        "plot_clusters(first_pass, 'cluster_1', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQSGOMiHfXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_centroids(df, column_header):\n",
        "  new_centroids = df.groupby(column_header).mean()\n",
        "  return new_centroids\n",
        "\n",
        "centroids = get_centroids(first_pass, 'cluster_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqzpUTU_e9Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(first_pass, 'cluster_1')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "second_pass = find_nearest_centroid(first_pass.select_dtypes(exclude='int64'), centroids, '2')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(second_pass, 'cluster_2', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzSUplKwCup",
        "colab_type": "text"
      },
      "source": [
        "## Use of sklearn.cluster.Kmeans and Practice\n",
        "You want the centroids based on a specified number of clusters? We got it here, and in just three lines of perfectly marinated code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I5xzWiwpcjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "13e731da-d820-4dde-e6d9-09e62f66555f"
      },
      "source": [
        "pc_df.head()\n",
        "print(B)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 9.18319983  1.97127137 -1.17162471 ... -1.37228049  0.37184363\n",
            "   2.18052922]\n",
            " [ 2.38329766 -3.75345877 -0.58022866 ... -0.12661612 -0.28826986\n",
            "   0.04493484]\n",
            " [ 5.74247239 -1.08035048 -0.53308788 ... -0.401855    0.4631876\n",
            "  -0.7152086 ]\n",
            " ...\n",
            " [ 1.2518901  -1.89397674  0.53446685 ...  0.39471327 -0.43780583\n",
            "  -0.17866837]\n",
            " [10.36503528  1.69639755 -1.90741785 ... -0.65671962 -0.19526395\n",
            "   0.27520221]\n",
            " [-5.47826365 -0.67278804  1.47716504 ... -1.57207609  0.60034041\n",
            "   1.63559151]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV9_k8mzoH6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e7fe6090-270f-48dd-cd4d-7a905456a3a8"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3).fit(B)\n",
        "kmeans.cluster_centers_\n",
        "\n",
        "# I'm sorry.. so what was the holdup? Is this really all that's wanted? The cluster centers; the centroids..\n",
        "# O how I feel emboldened\n",
        "\n",
        "# Find output below, three centroids with the coordinates of their respective eighth-dimensional locations. \n",
        "#  "
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.30457367e+00, -2.56926158e-01,  7.35271687e-02,\n",
              "         4.26245439e-02, -2.92506254e-02, -5.22700176e-02,\n",
              "         4.97783035e-03,  1.40573725e-02],\n",
              "       [ 2.35114517e+00,  2.96310426e+00, -7.20188863e-01,\n",
              "        -2.93613434e-01,  2.98880427e-01,  1.89113110e-01,\n",
              "         8.75283977e-02, -4.33864096e-02],\n",
              "       [ 5.34156103e+00, -1.88449060e+00,  4.21966807e-01,\n",
              "         1.30867001e-01, -1.79229084e-01, -3.52507079e-03,\n",
              "        -9.65675585e-02, -5.91374521e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Zb9od1tRA6",
        "colab_type": "text"
      },
      "source": [
        "So tell me Johnny, what's the big take-away?\n",
        "All we got are three locations in a higher-dimensional space. How do we interpret that? \n",
        "Well Moris, I ain't too sure, but I think what we have here, is a fail.. a failure...\n",
        "No, a *group* of clusters. \n",
        "\n",
        "In all seriousness, these clusters/groupings of datapoints should *explain* different observed outputs (the y vars). In this case, the y-vars or labels column is that attribute of the original imported df titled 'diagnosis'. How odd it is, if there are indeed only two possible outputs, that there would be needed three or more eighth-dimensional clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtjVij-3wMR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler  -> Rudy! Thanks for the help and sharing your methods\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# scaler.fit(df)\n",
        "# scaled_data = scaler.transform(df)\n",
        "# pca = PCA(n_components=2)\n",
        "# pca.fit(scaled_data)\n",
        "# x_pca = pca.transform(scaled_data)\n",
        "\n",
        "# plt.figure(figsize=(8,6))\n",
        "# plt.scatter(x_pca[:,0], x_pca[:,1])\n",
        "# plt.xlabel('First Principle Component')\n",
        "# plt.ylabel('Second Principle Component')\n",
        "# kmeans = KMeans(n_clusters=3)\n",
        "# kmeans.fit(x_pca)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBwVaGOOYsq",
        "colab_type": "text"
      },
      "source": [
        "# Stretch Goal:\n",
        "\n",
        "Once you are satisfied with your clustering, go back and add back in the labels from the original dataset to check how accurate your clustering was. Remember that this will not be a possibility in true unsupervised learning, but it might be a helpful for your learning to be able to check your work against the \"ground truth\". Try different approaches and see which one is the most successful and try understand why that might be the case. If you go back and try different methods don't ever include the actual \"diagnosis\" labels in your clustering or PCA.\n",
        "\n",
        "**Side Note** Data Science is never DONE. You just reach a point where the cost isn't worth the benefit anymore. There's always more moderate to small improvements that we could make. Don't be a perfectionist, be a pragmatist."
      ]
    }
  ]
}